#+SETUPFILE: ~/.emacs.d/themes/org-html-themes/setup/theme-readtheorg.setup
#+OPTIONS: \n:t
#+OPTIONS: ^:nil
#+OPTIONS: tex:t
#+STARTUP: latexpreview
#+OPTIONS: tex:dvipng
#+HTML_MATHJAX: align: left indent: 5em tagside: left font: Neo-Euler
#+attr_html: :width 300px
#+attr_latex: :width 300px
#+ATTR_ORG: :width 300

#+TITLE: linux内核设计与实现 - 基于x86 linux2.6


* 第一章 linux内核简介

内核是一个不可分割的静态可执行文件

- 单内核 - 单独的大过程，功能实现基本是函数调用，linux
- 微内核 - 分割为多个独立的过程，少数运行在特权模式，过程间用IPC通信，但实际应用中由于开销比较大，向单内核靠拢

linux版本 主版本号.从版本号.修订版本号.稳定版本号，其中 *从版本号偶数为稳定版*

上下文:
- 运行于用户态，执行用户进程
- 运行于内核空间，处于进程上下文，代表某个特定的进程执行 (int 0x80)
- 运行于内核空间，处于中断上下文，与任何进程无关，处理特定中断， *实际是用了被中断进程的内核栈*

* 第二章 从内核出发

*git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6.git*
e40152ee1e1c7a63f4777791863215e3faa37a86

iso: http://mirrors.ustc.edu.cn/ubuntu-old-releases/releases/10.04.0/ubuntu-10.04-desktop-i386.iso

#+begin_src txt
###### Ubuntu Main Repos
######

deb http://old-releases.ubuntu.com/ubuntu/ lucid main restricted
deb-src http://old-releases.ubuntu.com/ubuntu/ lucid main restricted
#+end_src

由于linux-2.6内核相对于现在来说很老了，最新的gcc glibc编译老的2.6内核会有各种问题，所以用老的基于linux2.6的系统最为方便

- 软件包推荐
| Name           | Description         |
|----------------+---------------------|
| openssh-server | ssh server          |
| vion           | VNC server，自带    |
| ncurses-dev    | make menuconfig所需 |

- 内核配置
| make config       | 遍历所有配置项 |
| *make menuconfig* | 图形界面工具   |
| make defconfig    | 缺省配置       |
| make oldconfig    | 验证和更新配置 |

- 拷贝本系统配置
#+begin_src bash
  zcat /proc/config.gz > .config  # 如果系统已开启CONFIG_IKCONFIG_PROC
  make oldconfig
#+end_src

- 内核编译
.config配置好以后, 执行make

- 内核安装
1. 生成的arch/i386/boot/bzImage拷到/boot下
2. make modules_install

内核开发的特点
- 无libc库或无标准头文件
- 经常使用编译器扩展特性
内联函数
内联汇编
链接脚本
关于时间戳，是64位的，但是经常也用到高32位，为了防止每次都额外做一次位运算，定义两个符号，指向同一个地址，一个是32位的，一个是64位
likely和unlikely
- 无内存保护
- 浮点
以前的cpu做浮点运算都有个协处理器，这样会使CPU陷入，所以尽量不要用浮点
- 内核栈大小有限，32位机默认是8KB
- *同步和并发* 开了内核抢占和非抢占时锁的行为和中断发生的时为不一样，要特别注意
- 可移植性，尽量做到体系无关

* 第三章 进程管理

- 进程列表存放在任务队列的双向循环链表中 - task list
- task_struct指针在thread_info中，thread_info在内核栈栈底，这样获取thread_info地址简单的方法就是esp & (0xFFFFFFFF << 13)，task_struct就是(esp & (0xFFFFFFFF << 13))->task

*像POWERPC, task_struct指针直接存在r2，相对的说原因之一就是x86的寄存器不够用*

- 默认最大PID是32768
- 进程状态
| TASK_RUNNINNG        | 运行                         |
| TASK_INNTERRUPTIBLE  | 可中断，正在睡眠             |
| TASK_UNINTERRUPTIBLE | 类似可中断，但对信号不做响应 |
| __TASK_TRACED        | 被其他进程跟踪               |
| __TASK_STOPPED       | 进程停止执行                 |
- task->state是进程状态
- 所有进程都是PID为1的init进程的后代， *而init进程的相关数据是静态写在kernel代码里的*, parent是父进程，childern是子进程链表，sibling是兄弟进程链表，同时所有进程在双向链表tasks中
#+begin_src c
struct task_struct {
	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
	void *stack;
	atomic_t usage;
	unsigned int flags;	/* per process flags, defined below */
	unsigned int ptrace;

	int lock_depth;		/* BKL lock depth */

#ifdef CONFIG_SMP
#ifdef __ARCH_WANT_UNLOCKED_CTXSW
	int oncpu;
#endif
#endif

	int prio, static_prio, normal_prio;
	unsigned int rt_priority;
	const struct sched_class *sched_class;
	struct sched_entity se;
	struct sched_rt_entity rt;

#ifdef CONFIG_PREEMPT_NOTIFIERS
	/* list of struct preempt_notifier: */
	struct hlist_head preempt_notifiers;
#endif

	/*
	 * fpu_counter contains the number of consecutive context switches
	 * that the FPU is used. If this is over a threshold, the lazy fpu
	 * saving becomes unlazy to save the trap. This is an unsigned char
	 * so that after 256 times the counter wraps and the behavior turns
	 * lazy again; this to deal with bursty apps that only use FPU for
	 * a short time
	 */
	unsigned char fpu_counter;
#ifdef CONFIG_BLK_DEV_IO_TRACE
	unsigned int btrace_seq;
#endif

	unsigned int policy;
	cpumask_t cpus_allowed;

#ifdef CONFIG_TREE_PREEMPT_RCU
	int rcu_read_lock_nesting;
	char rcu_read_unlock_special;
	struct rcu_node *rcu_blocked_node;
	struct list_head rcu_node_entry;
#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */

#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
	struct sched_info sched_info;
#endif

	struct list_head tasks;
	struct plist_node pushable_tasks;

	struct mm_struct *mm, *active_mm;
#if defined(SPLIT_RSS_COUNTING)
	struct task_rss_stat	rss_stat;
#endif
/* task state */
	int exit_state;
	int exit_code, exit_signal;
	int pdeath_signal;  /*  The signal sent when the parent dies  */
	/* ??? */
	unsigned int personality;
	unsigned did_exec:1;
	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
				 * execve */
	unsigned in_iowait:1;


	/* Revert to default priority/policy when forking */
	unsigned sched_reset_on_fork:1;

	pid_t pid;
	pid_t tgid;

#ifdef CONFIG_CC_STACKPROTECTOR
	/* Canary value for the -fstack-protector gcc feature */
	unsigned long stack_canary;
#endif

	/* 
	 * pointers to (original) parent process, youngest child, younger sibling,
	 * older sibling, respectively.  (p->father can be replaced with 
	 * p->real_parent->pid)
	 */
	struct task_struct *real_parent; /* real parent process */
	struct task_struct *parent; /* recipient of SIGCHLD, wait4() reports */
	/*
	 * children/sibling forms the list of my natural children
	 */
	struct list_head children;	/* list of my children */
	struct list_head sibling;	/* linkage in my parent's children list */
	struct task_struct *group_leader;	/* threadgroup leader */

	/*
	 * ptraced is the list of tasks this task is using ptrace on.
	 * This includes both natural children and PTRACE_ATTACH targets.
	 * p->ptrace_entry is p's link on the p->parent->ptraced list.
	 */
	struct list_head ptraced;
	struct list_head ptrace_entry;

	/*
	 * This is the tracer handle for the ptrace BTS extension.
	 * This field actually belongs to the ptracer task.
	 */
	struct bts_context *bts;

	/* PID/PID hash table linkage. */
	struct pid_link pids[PIDTYPE_MAX];
	struct list_head thread_group;

	struct completion *vfork_done;		/* for vfork() */
	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */

	cputime_t utime, stime, utimescaled, stimescaled;
	cputime_t gtime;
#ifndef CONFIG_VIRT_CPU_ACCOUNTING
	cputime_t prev_utime, prev_stime;
#endif
	unsigned long nvcsw, nivcsw; /* context switch counts */
	struct timespec start_time; 		/* monotonic time */
	struct timespec real_start_time;	/* boot based time */
/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
	unsigned long min_flt, maj_flt;

	struct task_cputime cputime_expires;
	struct list_head cpu_timers[3];

/* process credentials */
	const struct cred *real_cred;	/* objective and real subjective task
					 * credentials (COW) */
	const struct cred *cred;	/* effective (overridable) subjective task
					 * credentials (COW) */
	struct mutex cred_guard_mutex;	/* guard against foreign influences on
					 * credential calculations
					 * (notably. ptrace) */
	struct cred *replacement_session_keyring; /* for KEYCTL_SESSION_TO_PARENT */

	char comm[TASK_COMM_LEN]; /* executable name excluding path
				     - access with [gs]et_task_comm (which lock
				       it with task_lock())
				     - initialized normally by setup_new_exec */
/* file system info */
	int link_count, total_link_count;
#ifdef CONFIG_SYSVIPC
/* ipc stuff */
	struct sysv_sem sysvsem;
#endif
#ifdef CONFIG_DETECT_HUNG_TASK
/* hung task detection */
	unsigned long last_switch_count;
#endif
/* CPU-specific state of this task */
	struct thread_struct thread;
/* filesystem information */
	struct fs_struct *fs;
/* open file information */
	struct files_struct *files;
/* namespaces */
	struct nsproxy *nsproxy;
/* signal handlers */
	struct signal_struct *signal;
	struct sighand_struct *sighand;

	sigset_t blocked, real_blocked;
	sigset_t saved_sigmask;	/* restored if set_restore_sigmask() was used */
	struct sigpending pending;

	unsigned long sas_ss_sp;
	size_t sas_ss_size;
	int (*notifier)(void *priv);
	void *notifier_data;
	sigset_t *notifier_mask;
	struct audit_context *audit_context;
#ifdef CONFIG_AUDITSYSCALL
	uid_t loginuid;
	unsigned int sessionid;
#endif
	seccomp_t seccomp;

/* Thread group tracking */
   	u32 parent_exec_id;
   	u32 self_exec_id;
/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,
 * mempolicy */
	spinlock_t alloc_lock;

#ifdef CONFIG_GENERIC_HARDIRQS
	/* IRQ handler threads */
	struct irqaction *irqaction;
#endif

	/* Protection of the PI data structures: */
	raw_spinlock_t pi_lock;

#ifdef CONFIG_RT_MUTEXES
	/* PI waiters blocked on a rt_mutex held by this task */
	struct plist_head pi_waiters;
	/* Deadlock detection and priority inheritance handling */
	struct rt_mutex_waiter *pi_blocked_on;
#endif

#ifdef CONFIG_DEBUG_MUTEXES
	/* mutex deadlock detection */
	struct mutex_waiter *blocked_on;
#endif
#ifdef CONFIG_TRACE_IRQFLAGS
	unsigned int irq_events;
	unsigned long hardirq_enable_ip;
	unsigned long hardirq_disable_ip;
	unsigned int hardirq_enable_event;
	unsigned int hardirq_disable_event;
	int hardirqs_enabled;
	int hardirq_context;
	unsigned long softirq_disable_ip;
	unsigned long softirq_enable_ip;
	unsigned int softirq_disable_event;
	unsigned int softirq_enable_event;
	int softirqs_enabled;
	int softirq_context;
#endif
#ifdef CONFIG_LOCKDEP
# define MAX_LOCK_DEPTH 48UL
	u64 curr_chain_key;
	int lockdep_depth;
	unsigned int lockdep_recursion;
	struct held_lock held_locks[MAX_LOCK_DEPTH];
	gfp_t lockdep_reclaim_gfp;
#endif

/* journalling filesystem info */
	void *journal_info;

/* stacked block device info */
	struct bio_list *bio_list;

/* VM state */
	struct reclaim_state *reclaim_state;

	struct backing_dev_info *backing_dev_info;

	struct io_context *io_context;

	unsigned long ptrace_message;
	siginfo_t *last_siginfo; /* For ptrace use.  */
	struct task_io_accounting ioac;
#if defined(CONFIG_TASK_XACCT)
	u64 acct_rss_mem1;	/* accumulated rss usage */
	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
	cputime_t acct_timexpd;	/* stime + utime since last update */
#endif
#ifdef CONFIG_CPUSETS
	nodemask_t mems_allowed;	/* Protected by alloc_lock */
	int cpuset_mem_spread_rotor;
#endif
#ifdef CONFIG_CGROUPS
	/* Control Group info protected by css_set_lock */
	struct css_set *cgroups;
	/* cg_list protected by css_set_lock and tsk->alloc_lock */
	struct list_head cg_list;
#endif
#ifdef CONFIG_FUTEX
	struct robust_list_head __user *robust_list;
#ifdef CONFIG_COMPAT
	struct compat_robust_list_head __user *compat_robust_list;
#endif
	struct list_head pi_state_list;
	struct futex_pi_state *pi_state_cache;
#endif
#ifdef CONFIG_PERF_EVENTS
	struct perf_event_context *perf_event_ctxp;
	struct mutex perf_event_mutex;
	struct list_head perf_event_list;
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *mempolicy;	/* Protected by alloc_lock */
	short il_next;
#endif
	atomic_t fs_excl;	/* holding fs exclusive resources */
	struct rcu_head rcu;

	/*
	 * cache last used pipe for splice
	 */
	struct pipe_inode_info *splice_pipe;
#ifdef	CONFIG_TASK_DELAY_ACCT
	struct task_delay_info *delays;
#endif
#ifdef CONFIG_FAULT_INJECTION
	int make_it_fail;
#endif
	struct prop_local_single dirties;
#ifdef CONFIG_LATENCYTOP
	int latency_record_count;
	struct latency_record latency_record[LT_SAVECOUNT];
#endif
	/*
	 * time slack values; these are used to round up poll() and
	 * select() etc timeout values. These are in nanoseconds.
	 */
	unsigned long timer_slack_ns;
	unsigned long default_timer_slack_ns;

	struct list_head	*scm_work_list;
#ifdef CONFIG_FUNCTION_GRAPH_TRACER
	/* Index of current stored address in ret_stack */
	int curr_ret_stack;
	/* Stack of return addresses for return function tracing */
	struct ftrace_ret_stack	*ret_stack;
	/* time stamp for last schedule */
	unsigned long long ftrace_timestamp;
	/*
	 * Number of functions that haven't been traced
	 * because of depth overrun.
	 */
	atomic_t trace_overrun;
	/* Pause for the tracing */
	atomic_t tracing_graph_pause;
#endif
#ifdef CONFIG_TRACING
	/* state flags for use by tracers */
	unsigned long trace;
	/* bitmask of trace recursion */
	unsigned long trace_recursion;
#endif /* CONFIG_TRACING */
#ifdef CONFIG_CGROUP_MEM_RES_CTLR /* memcg uses this to do batch job */
	struct memcg_batch_info {
		int do_batch;	/* incremented when batch uncharge started */
		struct mem_cgroup *memcg; /* target memcg of uncharge */
		unsigned long bytes; 		/* uncharged usage */
		unsigned long memsw_bytes; /* uncharged mem+swap usage */
	} memcg_batch;
#endif
};

struct thread_info {
	struct task_struct	*task;		/* main task structure */
	struct exec_domain	*exec_domain;	/* execution domain */
	__u32			flags;		/* low level flags */
	__u32			status;		/* thread synchronous flags */
	__u32			cpu;		/* current CPU */
	int			preempt_count;	/* 0 => preemptable,
						   <0 => BUG */
	mm_segment_t		addr_limit;
	struct restart_block    restart_block;
	void __user		*sysenter_return;
#ifdef CONFIG_X86_32
	unsigned long           previous_esp;   /* ESP of the previous stack in
						   case of nested (IRQ) stacks
						*/
	__u8			supervisor_stack[0];
#endif
	int			uaccess_err;
};
#+end_src
- 写时拷贝,页权限设为只读，真正要写的时候再拷贝新页

- 用户态fork -> 用户态clone -> 内核态do_fork -> copy_process:
dup_task_struct
检查进程资源限制
子进程task_struct某些信息清0
子进程state = UNINTERRUPTIBLE
copy_flags
alloc_pid申请子进程pid
资源分配
- vfork为了那些马上exec的进程使用，不推荐
- linux进程与线程的区别只在与是否共享某些资源

| CLONE 参数标志       | 含义                                 |
|----------------------+--------------------------------------|
| CLONE_FILES          | 共享打开的文件                       |
| CLONE_FS             | 共享文件系统信息                     |
| CLONE_IDLETASK       | PID设为0(dedicated for init process) |
| CLONE_NEWNS          | 子进程有新的命令空间                 |
| CLONE_PARENT         | 子进程与父进程拥有同一个父进程       |
| CLONE_PTRACE         | 调试子进程，gdb会用这个              |
| CLONE_SETTID         | 将TID回写至用户空间                  |
| CLONE_SETTLS         | 为子进程创建新的TLS                  |
| CLONE_SIGHAND        | 共享信号处理函数及被阻断的信号       |
| CLONE_SYSVEM         | 共享SytemV SEM_UNDO语义              |
| CLONE_THREAD         | 相同的线程组                         |
| CLONE_VFORK          | vfork()使用                          |
| CLONE_UNTRACED       | 防止被trace,主要是防止跟踪           |
| CLONE_STOP           | 以TASK_STOPPED状态开始进程           |
| CLONE_CHILD_CLEARTID | 清除子进程的TID                      |
| CLONE_CHILD_SETTID   | 设置子进程的TID                      |
| CLONE_PARENT_SETTID  | 设置父进程的TID                      |
| CLONE_VM             | *共享地址空间*                       |

- 内核线程，没用用户空间的线程， mm为NULL, 相关头文件为kthread.h
- exit -> do_exit，永不返回
task_struct标志PF_EXITING
del_timer_sync
exit_sem, exit_mm, exit_files, exit_fs
设置exit_code
exit_notify让其父进程为其子进程重新设置父进程，同时设状态为ZOMBIE
schedule，父进程通过wait帮其清理内核栈, thread_info，task_struct
- wait
为退出进程的子进程找新的父进程
为被退出进程trace的进程找新的父进程

* 第四章 进程调度

- preemption 内核调度程序决定进程挂起与运行
- yielding   进程本身主动挂起
- 进程调度在响应时间和吞吐量之间做平衡
- 传统的绝对时间片会引发的固定的切换频率问题，linux使用了公平调度
se 是调度器实体
vruntime 虚拟实时，系统定时器周期性调用update_curr()更新

#+begin_src c
struct sched_entity {
	struct load_weight	load;		/* for load-balancing */
	struct rb_node		run_node;
	struct list_head	group_node;
	unsigned int		on_rq;

	u64			exec_start;
	u64			sum_exec_runtime;
	u64			vruntime;
	u64			prev_sum_exec_runtime;

	u64			last_wakeup;
	u64			avg_overlap;

	u64			nr_migrations;

	u64			start_runtime;
	u64			avg_wakeup;
//...
};

static void update_curr(struct cfs_rq *cfs_rq)
{
	struct sched_entity *curr = cfs_rq->curr;
	u64 now = rq_of(cfs_rq)->clock;
	unsigned long delta_exec;

	if (unlikely(!curr))
		return;

	/*
	 * Get the amount of time the current task was running
	 * since the last time we changed load (this cannot
	 * overflow on 32 bits):
	 */
	delta_exec = (unsigned long)(now - curr->exec_start);
	if (!delta_exec)
		return;

	__update_curr(cfs_rq, curr, delta_exec);
	curr->exec_start = now;

	if (entity_is_task(curr)) {
		struct task_struct *curtask = task_of(curr);

		trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
		cpuacct_charge(curtask, delta_exec);
		account_group_exec_runtime(curtask, delta_exec);
	}
}

static inline void
__update_curr(struct cfs_rq *cfs_rq, struct sched_entity *curr,
	      unsigned long delta_exec)
{
	unsigned long delta_exec_weighted;

	schedstat_set(curr->exec_max, max((u64)delta_exec, curr->exec_max));

	curr->sum_exec_runtime += delta_exec;
	schedstat_add(cfs_rq, exec_clock, delta_exec);
	delta_exec_weighted = calc_delta_fair(delta_exec, curr);

	curr->vruntime += delta_exec_weighted;
	update_min_vruntime(cfs_rq);
}

#+end_src

- 进程选择
红黑树
enqueue_entity +进程
dequeue_entity -进程
#+begin_src c
static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq)
{
	struct rb_node *left = cfs_rq->rb_leftmost;

	if (!left)
		return NULL;

	return rb_entry(left, struct sched_entity, run_node);
}


#+end_src

- 调度器函数 schedule->pick_next_task，会从高优先级到低优先级调度器类中找第一个进程
- 休眠
从可执行红黑树中移出，放入等待队列
#+begin_src c
DEFINE_WAIT(wait); //创建队列项
add_wait_queue(q, &wait); //加入等待队列
while(!condition) {
//条件不满足
    prepare_to_wait(&q, &wait, TASK_INTERRUPTIBLE); //设置进程状态，如果此时已退出等待队列，重新加入
    if(signal_pending(current)) //处理信号
    schedule();//调度
}
finish_wait(&q, &wait);//结束等待
#+end_src
- 唤醒
从等待队列移到红黑树
- 上下文切换
switch_mm 切换虚拟内存映射
switch_to 切换寄存器组
- 用户抢占
发生在系统调用或中断处理程序返回用户空间
- 内核抢占
*非内核抢占的操作系统中，调度程序没有办法在一个内核级的任务正在执行的时候重新调度*
thread_info->preempt_count为0时，可抢占(此时是否重新调度取决于need_resched)
a. 中断处理程序返回内核空间
b. 内核代码再一次有抢占性
c. 显示调用schedule
d. 内核任务阻塞(一般是加到等待队列，同时调用schedule)
- 实时调度策略
在sched_rt.c中,之前的CFS在sched_fair.c(SCHED_NORMAL)中
a. SCHED_FIFO, 一直执行到主动放弃或被更高优先级抢占
b. SCHED_RR, 带时间片的SCHED_FIFO
实时优先级为静态优先级，不像普通进程会动态计算优先级
- 优先级范围
nice -20~+19 相当于 实时优先级 100~139
#+begin_src c
/*
 * Priority of a process goes from 0..MAX_PRIO-1, valid RT
 * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
 * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
 * values are inverted: lower p->prio value means higher priority.
 *
 * The MAX_USER_RT_PRIO value allows the actual maximum
 * RT priority to be separate from the value exported to
 * user-space.  This allows kernel threads to set their
 * priority to a value higher than any user task. Note:
 * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
 */

#define MAX_USER_RT_PRIO	100
#define MAX_RT_PRIO		MAX_USER_RT_PRIO

#define MAX_PRIO		(MAX_RT_PRIO + 40)
#define DEFAULT_PRIO		(MAX_RT_PRIO + 20)
#+end_src

* 第五章 系统调用

除异常和陷入外内核唯一的合法入口
*当进程从用户态切换至内核栈时,X86会进行栈切换(取出tss段中的esp0)*
*需保证系统调用是可重入的,因为进程上下文中，内核可被抢占，所以同时可能存在多个相同的系统调用*
*反过来说中断处理程序不能休眠*

*asmlinkage* 限制词表示用栈传数据
sys_call_table 是内核的系统调用表

#+begin_src c
//entry_32.S
syscall_call:
	call *sys_call_table(,%eax,4)
	movl %eax,PT_EAX(%esp)		# store the return value

//entry_64.S
ENTRY(system_call)
	CFI_STARTPROC	simple
	CFI_SIGNAL_FRAME
	CFI_DEF_CFA	rsp,KERNEL_STACK_OFFSET
	CFI_REGISTER	rip,rcx
	/*CFI_REGISTER	rflags,r11*/
	SWAPGS_UNSAFE_STACK
#+end_src

内核和用户空间间做数据传输时有可能会引起阻塞（所需页在磁盘上）
- copy_from_user 从用户空间拷贝
- copy_to_user   拷贝到用户空间

capable可检查是否可操作指定资源

** 新增系统调用
*** 环境
- linux kernel
a. make defconfig
b. make menuconfig
#+begin_src bash
# make menuconfig
[ ] Network packet filtering framework (Netfilter)  --->
#+end_src
c. make
- busybox
*git://git.busybox.net/busybox*
e50f74da70da645c25d7daa81b2d9796a738f718 tag: 1_24_2

a. make menuconfig
#+begin_src bash
# make menuconfig
[*] Don't use /usr
[*] Build BusyBox as a static binary (no shared libs)
[ ] sync
#+end_src
b. make
c. make install
- initramfs
a. 建立目录
#+begin_src bash
#!/bin/bash
mkdir -pv initramfs/x86-busybox
cd initramfs/x86-busybox
mkdir -pv {bin,sbin,etc,proc,sys,usr/{bin,sbin}}
cp -av /mnt/hgfs/linux-2.6-git/busybox/_install/* .
#+end_src
b. 在目录下建立init
#+begin_src sh
#!/bin/sh
mount -t proc none /proc
mount -t sysfs none /sys
echo -e "\nBoot took $(cut -d' ' -f1 /proc/uptime) seconds\n"
exec /bin/sh
#+end_src
c. 生成initramfs
在目录下
find . -print0 | cpio --null -ov --format=newc | gzip -9 > x86.cpio.gz
- qemu
qemu -kernel bzImage -initrd ./x86.cpio.gz -nographic -append "console=ttyS0"

*** sys_foo
- kernel
#+begin_src asm
; arch/x86/kernel/syscall_table_32.S
  ...
      .long sys_rt_tgsigqueueinfo	/* 335 */
      .long sys_perf_event_open
      .long sys_recvmmsg
      .long sys_foo               ;新增338
  ...
#+end_src
#+begin_src c
//kernel/sys.c
asmlinkage long sys_foo(void)
{
    return 0xabcd;
}
#+end_src
- app
#+begin_src c
  /* foo_app.c : gcc -static foo_app.c -o foo_app*/
  #include <stdio.h>
  #include <syscall.h>

  #define __NR_foo 338
  int main(int argc, char *argv[])
  {
      printf("new sys_call return 0x%x\n", syscall(__NR_foo));
      return 0;
  }
#+end_src
#+begin_src bash
#老的kernel
new sys_call return 0xffffffff

#修改过的kernel
new sys_call return 0xabcd
#+end_src

* 第六章 内核数据结构
** 链表
linux/list.h
#+begin_src c
  struct list_head {
      struct list_head *next, *prev;
  };

  #define LIST_HEAD_INIT(name) { &(name), &(name) } //静态声明
  #define LIST_HEAD(name) \
      struct list_head name = LIST_HEAD_INIT(name)

  static inline void INIT_LIST_HEAD(struct list_head *list) //动态
  {
      list->next = list;
      list->prev = list;
  }

  static inline void __list_add(struct list_head *new,
                    struct list_head *prev,
                    struct list_head *next)
  {
      next->prev = new;
      new->next = next;
      new->prev = prev;
      prev->next = new;
  }

  //加元素，在head后面
  static inline void list_add(struct list_head *new, struct list_head *head)
  {
      __list_add(new, head, head->next);
  }

  //加元素，在链表尾
  static inline void list_add_tail(struct list_head *new, struct list_head *head)
  {
      __list_add(new, head->prev, head);
  }

  static inline void __list_del(struct list_head * prev, struct list_head * next)
  {
      next->prev = prev;
      prev->next = next; //互相挂接
  }

  //删除元素
  static inline void list_del(struct list_head *entry)
  {
      __list_del(entry->prev, entry->next);
      entry->next = LIST_POISON1;
      entry->prev = LIST_POISON2;
  }


  static inline void list_move(struct list_head *list, struct list_head *head)
  {
      __list_del(list->prev, list->next); //list从当前链表退出
      list_add(list, head); //加到head后
  }

  static inline void list_move_tail(struct list_head *list,
                    struct list_head *head)
  {
      __list_del(list->prev, list->next);
      list_add_tail(list, head); //加到head->prev后
  }


  /* <= head <=> head1 <=> head2 <=> .... head_end => */
  /* <= list <=> list1 <=> list2 <=> .... list_end => */

  /* <= head <=> list1 <=> list2 <=> .... list_end <=> head1 <=> head2 ... head_end => */
  static inline void __list_splice(const struct list_head *list,
                   struct list_head *prev,
                   struct list_head *next)
  {
      struct list_head *first = list->next;
      struct list_head *last = list->prev;

      first->prev = prev;
      prev->next = first;         /* head <=> list->next */

      last->next = next;          /* head->next <=> list->prev*/
      next->prev = last;
  }

  static inline void list_splice(const struct list_head *list,
                  struct list_head *head)
  {
      if (!list_empty(list))
          __list_splice(list, head, head->next);
  }

/**
 * list_for_each	-	iterate over a list
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */
//遍历
 #define list_for_each(pos, head) \
	for (pos = (head)->next; prefetch(pos->next), pos != (head); \
        	pos = pos->next)

/**
 * list_entry - get the struct for this entry
 * @ptr:	the &struct list_head pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_struct within the struct.
 */
 #define list_entry(ptr, type, member) \
	container_of(ptr, type, member)

/**
 * list_for_each_entry	-	iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 */
 #define list_for_each_entry(pos, head, member)				\
	for (pos = list_entry((head)->next, typeof(*pos), member);	\
	     prefetch(pos->member.next), &pos->member != (head); 	\
	     pos = list_entry(pos->member.next, typeof(*pos), member))

/**
 * list_for_each_safe - iterate over a list safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */
//安全遍历，用于在foreach中删除元素
#define list_for_each_safe(pos, n, head) \
	for (pos = (head)->next, n = pos->next; pos != (head); \
		pos = n, n = pos->next)
#+end_src

** 数据队列
linux/kfifo.h
#+begin_src c
#define __kfifo_initializer(s, b) \
	(struct kfifo) { \
		.size	= s, \
		.in	= 0, \
		.out	= 0, \
		.buffer = b \
	}

//联合体，头为kfifo结构，后面为buffer size
#define DECLARE_KFIFO(name, size) \
union { \
	struct kfifo name; \
	unsigned char name##kfifo_buffer[size + sizeof(struct kfifo)]; \
}
/**
 * INIT_KFIFO - Initialize a kfifo declared by DECLARE_KFIFO
 * @name: name of the declared kfifo datatype
 */
#define INIT_KFIFO(name) \
	name = __kfifo_initializer(sizeof(name##kfifo_buffer) - \
				sizeof(struct kfifo), \
				name##kfifo_buffer + sizeof(struct kfifo))

//分离的结构，一个是kfifo，挂接buffer
#define DEFINE_KFIFO(name, size) \
	unsigned char name##kfifo_buffer[size]; \
	struct kfifo name = __kfifo_initializer(size, name##kfifo_buffer)

/**
 * kfifo_alloc - allocates a new FIFO internal buffer
 * @fifo: the fifo to assign then new buffer
 * @size: the size of the buffer to be allocated, this have to be a power of 2.
 * @gfp_mask: get_free_pages mask, passed to kmalloc()
 *
 * This function dynamically allocates a new fifo internal buffer
 *
 * The size will be rounded-up to a power of 2.
 * The buffer will be release with kfifo_free().
 * Return 0 if no error, otherwise the an error code
 */
//申请
int kfifo_alloc(struct kfifo *fifo, unsigned int size, gfp_t gfp_mask)

//推入
/**
 * kfifo_in - puts some data into the FIFO
 * @fifo: the fifo to be used.
 * @from: the data to be added.
 * @len: the length of the data to be added.
 *
 * This function copies at most @len bytes from the @from buffer into
 * the FIFO depending on the free space, and returns the number of
 * bytes copied.
 *
 * Note that with only one concurrent reader and one concurrent
 * writer, you don't need extra locking to use these functions.
 */
unsigned int kfifo_in(struct kfifo *fifo, const void *from,
				unsigned int len)


//取出
/**
 * kfifo_out - gets some data from the FIFO
 * @fifo: the fifo to be used.
 * @to: where the data must be copied.
 * @len: the size of the destination buffer.
 *
 * This function copies at most @len bytes from the FIFO into the
 * @to buffer and returns the number of copied bytes.
 *
 * Note that with only one concurrent reader and one concurrent
 * writer, you don't need extra locking to use these functions.
 */
unsigned int kfifo_out(struct kfifo *fifo, void *to, unsigned int len)
#+end_src

** 映射
linux/idr.h
不像python里的dict,key就是UID，是自动生成的
#+begin_src c
//初始化
/**
 * idr_init - initialize idr handle
 * @idp:	idr handle
 *
 * This function is use to set up the handle (@idp) that you will pass
 * to the rest of the functions.
 */
void idr_init(struct idr *idp)

//预分配UID
/**
 * idr_pre_get - reserver resources for idr allocation
 * @idp:	idr handle
 * @gfp_mask:	memory allocation flags
 *
 * This function should be called prior to locking and calling the
 * idr_get_new* functions. It preallocates enough memory to satisfy
 * the worst possible allocation.
 *
 * If the system is REALLY out of memory this function returns 0,
 * otherwise 1.
 */
int idr_pre_get(struct idr *idp, gfp_t gfp_mask)

//映射
/**
 * idr_get_new - allocate new idr entry
 * @idp: idr handle
 * @ptr: pointer you want associated with the id
 * @id: pointer to the allocated handle
 *
 * This is the allocate id function.  It should be called with any
 * required locks.
 *
 * If memory is required, it will return -EAGAIN, you should unlock
 * and go back to the idr_pre_get() call.  If the idr is full, it will
 * return -ENOSPC.
 *
 * @id returns a value in the range 0 ... 0x7fffffff
 */
int idr_get_new(struct idr *idp, void *ptr, int *id)

//根据UID取ptr
/**
 * idr_find - return pointer for given id
 * @idp: idr handle
 * @id: lookup key
 *
 * Return the pointer given the id it has been registered with.  A %NULL
 * return indicates that @id is not valid or you passed %NULL in
 * idr_get_new().
 *
 * This function can be called under rcu_read_lock(), given that the leaf
 * pointers lifetimes are correctly managed.
 */
void *idr_find(struct idr *idp, int id)

//删除UID
/**
 * idr_remove - remove the given id and free it's slot
 * @idp: idr handle
 * @id: unique key
 */
void idr_remove(struct idr *idp, int id)

//删除idr
/**
 * idr_destroy - release all cached layers within an idr tree
 * idp: idr handle
 */
void idr_destroy(struct idr *idp)
#+end_src

** 红黑树
linux/rbtree.h
#+begin_src c
//初始化
struct rb_root root = RB_ROOT;

//红黑树没有插入和查找接口，需自已实现
static inline struct page * rb_search_page_cache(struct inode * inode,
						 unsigned long offset)
{
	struct rb_node * n = inode->i_rb_page_cache.rb_node;
	struct page * page;

	while (n)
	{
		page = rb_entry(n, struct page, rb_page_cache);

		if (offset < page->offset)
			n = n->rb_left;
		else if (offset > page->offset)
			n = n->rb_right;
		else
			return page;
	}
	return NULL;
}

static inline struct page * rb_insert_page_cache(struct inode * inode,
						 unsigned long offset,
						 struct rb_node * node)
{
	struct page * ret;
	if ((ret = __rb_insert_page_cache(inode, offset, node)))
		goto out;
	rb_insert_color(node, &inode->i_rb_page_cache);
 out:
	return ret;
}
#+end_src

* 第七章 中断和中断处理

中断可能随时发生，中断上下文不可阻塞

上半部 - 处理有严格时限的工作
下半部 - 处理大量逻辑的工作

request_irq里会调用kmalloc，有时会导致睡眠
#+begin_src c
/* irq     中断号 */
/* handler 中断处理程序 */
/* flags   中断处理标志 */
/* - IRQF_DISABLE       中断处理程序时禁止所有中断 */
/* - IRQF_SAMPLE_RANDOM 随机熵源 */
/* - IRQF_TIMER         专为系统定时器的中断处理 */
/* - IRQF_SHARED        共享中断线 */
/* name    ASCII文本 */
/* dev     共享中断时的priv指针 */
typedef irqreturn_t (*irq_handler_t)(int, void *);
static inline int __must_check
request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,
	    const char *name, void *dev)
{
	return request_threaded_irq(irq, handler, NULL, flags, name, dev);
}

extern void free_irq(unsigned int, void *);
#+end_src

*linux-的中断处理程序无须重入*
当一个中断执行时， +相应的中断号会在所有处理器上被屏蔽，同一个中断处理程充绝不会同时被调用以处理嵌套中断+ -> CPU在执行中断程序时硬件会关掉所有中断，最新的kernel在中断处理程序时不会开中断，所以执行中断的时候不会被其他中断程序打断


共享中断要求
1. IRQF_SHARED
2. dev参数唯一
3. 必须区分是自已的设备产生了中断，而不是共享了这个中断号的其他设备

中断上下文和进程无关，所以不可以睡眠，同时有严格的时间限制

中断流程
硬件产生一个中断 -> 中断控制器 -> 处理器 -> 处理器中断内核 -> do_IRQ() -> handle_IRQ_event()  +可能在此开启其他中断+ -> 运行挂接的所有中断处理程序 -> 关中断 -> ret_from_intr()

#+begin_src c
/*
 * do_IRQ handles all normal device IRQ's (the special
 * SMP cross-CPU interrupts have their own specific
 * handlers).
 */
unsigned int __irq_entry do_IRQ(struct pt_regs *regs)
{
	struct pt_regs *old_regs = set_irq_regs(regs);

	/* high bit used in ret_from_ code  */
	unsigned vector = ~regs->orig_ax;
	unsigned irq;

	exit_idle();
	irq_enter();

	irq = __get_cpu_var(vector_irq)[vector];

	if (!handle_irq(irq, regs)) {
		ack_APIC_irq();

		if (printk_ratelimit())
			pr_emerg("%s: %d.%d No irq handler for vector (irq %d)\n",
				__func__, smp_processor_id(), vector, irq);
	}

	irq_exit();

	set_irq_regs(old_regs);
	return 1;
}
#+end_src

#+begin_src asm
ret_from_intr:
	GET_THREAD_INFO(%ebp)
check_userspace:
	movl PT_EFLAGS(%esp), %eax	# mix EFLAGS and CS
	movb PT_CS(%esp), %al //检查栈中的cs，是否是用户权限来判断是否从用户态中断
	andl $(X86_EFLAGS_VM | SEGMENT_RPL_MASK), %eax
	cmpl $USER_RPL, %eax
	jb resume_kernel		# not returning to v8086 or userspace

ENTRY(resume_userspace)
	LOCKDEP_SYS_EXIT
 	DISABLE_INTERRUPTS(CLBR_ANY)	# make sure we don't miss an interrupt
					# setting need_resched or sigpending
					# between sampling and the iret
	TRACE_IRQS_OFF
	movl TI_flags(%ebp), %ecx
	andl $_TIF_WORK_MASK, %ecx	# is there any work to be done on
					# int/exception return?
	jne work_pending
	jmp restore_all
#+end_src

如下，第一列中断号，第二列是中断计数，第三列是中断控制器，第四列是中断名
#+begin_src bash
         CPU0       
  0:        268   IO-APIC-edge      timer
  1:       3261   IO-APIC-edge      i8042
  4:      31532   IO-APIC-edge    
  8:          1   IO-APIC-edge      rtc0
  9:          0   IO-APIC-fasteoi   acpi
 12:       6289   IO-APIC-edge      i8042
 14:          0   IO-APIC-edge      ata_piix
 15:          0   IO-APIC-edge      ata_piix
 16:     229317   IO-APIC-fasteoi   eth1
 17:      52749   IO-APIC-fasteoi   ioc0, Ensoniq AudioPCI
 18:        228   IO-APIC-fasteoi   ehci_hcd:usb1, uhci_hcd:usb2
#+end_src

中断控制
| Name               | 说明                                                     | 可否嵌套 |
|--------------------+----------------------------------------------------------+----------|
| local_irq_disable  | 禁止本地中断                                             | 否       |
| local_irq_enable   | 使能本地中断                                             | 否       |
| local_irq_save     | 保存本地中断状态，禁止本地中断                           | 是       |
| local_irq_restore  | 恢复之前的中断状态                                       | 是       |
| disable_irq        | 等待所有处理程序完毕，才关指点定中断线                   | 是       |
| disable_irq_nosync | 不等，直接关                                             | 是       |
| enable_irq         | 使能指定中断线                                           | N/A      |
| synchronize_irq    | 等待直到中断处理程序退出                                 | N/A      |
| irqs_disabled      | 本地中断是否关闭                                         | N/A      |
| in_interrupt       | 中断上下文为0(包括下半部，判断软件标记位)，进程上下文为1 | N/A      |
| in_irq             | 正在执行中断处理程为1                                    | N/A      |

* 第八章 下半部和推后执行的工作

| Name   | Policy           |
|--------+------------------|
| 上半部 | 对时间非常敏感   |
| 上半部 | 和硬件相关       |
| 上半部 | 保证不被其他中断 |
| 下半部 | 除开上面         |

下半部机制: 软中断、tasklets、工作队列

- 软中断 kernel/softirq.c
编译时静态分配

*注意，因为看pending的标志是32位的，所以最多有32个软中断*
#+begin_src c
enum
{
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	BLOCK_IOPOLL_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
	HRTIMER_SOFTIRQ,
	RCU_SOFTIRQ,	/* Preferable RCU should always be the last softirq */

	NR_SOFTIRQS
};

struct softirq_action
{
	void	(*action)(struct softirq_action *);
};

static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;

#+end_src

*一个软中断不会抢占另外一个软中断，唯一可抢占软中断的是中断处理程序*

软中断何时被运行:
1. 硬件中断代码返回时
2. ksoftirq内核线程中
3. 显示检查和执行待处理的软中断的代码中

#+begin_src c
asmlinkage void __do_softirq(void)
{
	struct softirq_action *h;
	__u32 pending;
	int max_restart = MAX_SOFTIRQ_RESTART;
	int cpu;

	pending = local_softirq_pending();

    ...

	h = softirq_vec;

	do {
		if (pending & 1) {
			int prev_count = preempt_count();
			kstat_incr_softirqs_this_cpu(h - softirq_vec);

			trace_softirq_entry(h, softirq_vec);
			h->action(h);


		h++;
		pending >>= 1;
	} while (pending);

    ...
}

asmlinkage void do_softirq(void)
{
	__u32 pending;
	unsigned long flags;

	if (in_interrupt())
		return;

	local_irq_save(flags);

	pending = local_softirq_pending();

	if (pending)
		__do_softirq();

	local_irq_restore(flags);
}
#+end_src

软中断执行过程中，允许响应中断，但自身不能休眠，通常来说软中断用于处理单处理器数据

- tasklet

源头是软中断的HI_SOFTIRQ和TASKLET_SOFTIRQ
#+begin_src c

enum
{
	TASKLET_STATE_SCHED,	/* Tasklet is scheduled for execution */
	TASKLET_STATE_RUN	/* Tasklet is running (SMP only) */
};

struct tasklet_struct
{
	struct tasklet_struct *next; //链表指针，头是task_vec和task_hi_vec
	unsigned long state; // TASKLET_STATE_RUN的话是看其他处理器是否在运行
	atomic_t count; //不为0表示tassklet被禁止
	void (*func)(unsigned long);
	unsigned long data;
};

//静态创建
#define DECLARE_TASKLET(name, func, data) \
struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }

#define DECLARE_TASKLET_DISABLED(name, func, data) \
struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }

//动态创建
void tasklet_init(struct tasklet_struct *t,
		  void (*func)(unsigned long), unsigned long data)
{
	t->next = NULL;
	t->state = 0;
	atomic_set(&t->count, 0);
	t->func = func;
	t->data = data;
}
#+end_src

加tasklet的函数是tasklet_schedule和tasklet_hi_vec，功能是加到tasklet_vec和tasklet_hi_vec链表中
执行tasklet的函数是tasklet_action和tasklet_hi_action，功能是遍历tasklet_vec和tasklet_hi_vec链表，执行action，同时如果tasklet_struct->count不为0，则重新加入链表

- ksoftirqd
立即处理所有软中断(包括重复自触发)         - 大量软中断存在时无法调度用户任务
下一次中断时处理所有软中断(包括重复自触发) - 系统空闲没有太多中断时影响响应时间

*每个处长理器创建最低优先级内核线程ksoftirqd/n*
#+begin_src c
static int run_ksoftirqd(void * __bind_cpu)
{
	set_current_state(TASK_INTERRUPTIBLE);

	while (!kthread_should_stop()) { //死循环
		preempt_disable();
		if (!local_softirq_pending()) { //无软中断则调度
			preempt_enable_no_resched();
			schedule();
			preempt_disable();
		}

		__set_current_state(TASK_RUNNING);

		while (local_softirq_pending()) {
			/* Preempt disable stops cpu going offline.
			   If already offline, we'll be on wrong CPU:
			   don't process */
			if (cpu_is_offline((long)__bind_cpu))
				goto wait_to_die;
			do_softirq(); //有软中断，处理
			preempt_enable_no_resched();
			cond_resched();
			preempt_disable();
			rcu_sched_qs((long)__bind_cpu);
		}
		preempt_enable();
		set_current_state(TASK_INTERRUPTIBLE);
	}
	__set_current_state(TASK_RUNNING);
	return 0;

wait_to_die:
	preempt_enable();
	/* Wait for kthread_stop */
	set_current_state(TASK_INTERRUPTIBLE);
	while (!kthread_should_stop()) {
		schedule();
		set_current_state(TASK_INTERRUPTIBLE);
	}
	__set_current_state(TASK_RUNNING);
	return 0;
}
#+end_src

- 工作队列
任务交由内核线程去执行，同时可以睡眠
可能自已创建一个内核线程，但最好使用默认建好的线程events/n
#+begin_src c
//工作者结构，可以有有多个实例，默认是events内核线程组
struct workqueue_struct {
	struct cpu_workqueue_struct *cpu_wq; //实际是个数组，每一项对应一个处理器
	struct list_head list;
	const char *name;
	int singlethread;
	int freezeable;		/* Freeze threads during suspend */
	int rt;
#ifdef CONFIG_LOCKDEP
	struct lockdep_map lockdep_map;
#endif
};

//每个CPU有一个内核线程，传进去的结构就是struct cpu_workqueue_struct
struct cpu_workqueue_struct {

	spinlock_t lock;

	struct list_head worklist;
	wait_queue_head_t more_work;
	struct work_struct *current_work;

	struct workqueue_struct *wq;
	struct task_struct *thread;
} ____cacheline_aligned;

//具体工作，挂在struct cpu_workqueue_struct->more_work
typedef void (*work_func_t)(struct work_struct *work);
struct work_struct {
	atomic_long_t data;
#define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
#define WORK_STRUCT_STATIC  1		/* static initializer (debugobjects) */
#define WORK_STRUCT_FLAG_MASK (3UL)
#define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
	struct list_head entry;
	work_func_t func;
#ifdef CONFIG_LOCKDEP
	struct lockdep_map lockdep_map;
#endif
};

//每个工作者线程都使用内核线程，并执行worker_thread
static int worker_thread(void *__cwq)
{
	struct cpu_workqueue_struct *cwq = __cwq;
	DEFINE_WAIT(wait);

	if (cwq->wq->freezeable)
		set_freezable();

	for (;;) {
		prepare_to_wait(&cwq->more_work, &wait, TASK_INTERRUPTIBLE);
		if (!freezing(current) &&
		    !kthread_should_stop() &&
		    list_empty(&cwq->worklist))
			schedule();
		finish_wait(&cwq->more_work, &wait);

		try_to_freeze();

		if (kthread_should_stop())
			break;

		run_workqueue(cwq);
	}

	return 0;
}

//工作初始化
#define DECLARE_WORK(n, f)					\
	struct work_struct n = __WORK_INITIALIZER(n, f)
#define INIT_WORK(_work, _func)					\
	do {							\
		__INIT_WORK((_work), (_func), 0);		\
	} while (0)

//插入events工作线程
int schedule_work(struct work_struct *work)
{
	return queue_work(keventd_wq, work);
}

//插入events工作线程 但经过delay后才可被执行
int schedule_delayed_work(struct delayed_work *dwork,
					unsigned long delay)
{
	return queue_delayed_work(keventd_wq, dwork, delay);
}

//等待所有非delay work工作都被执行后才返回，可能会休眠
void flush_scheduled_work(void)
{
	flush_workqueue(keventd_wq);
}

//取消延时执行的工作
int cancel_delayed_work_sync(struct delayed_work *dwork)
{
	return __cancel_work_timer(&dwork->work, &dwork->timer);
}

//创建自已的工作者结构，等同events/n
#define create_workqueue(name) __create_workqueue((name), 0, 0, 0)
//加入自定义的工作队列
extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
extern int queue_delayed_work(struct workqueue_struct *wq,
			struct delayed_work *work, unsigned long delay);
//刷新
extern void flush_workqueue(struct workqueue_struct *wq);
#+end_src


| 下半部   | 上下文 | 顺序执行保障       |
|----------+--------+--------------------|
| 软中断   | 中断   | 没有               |
| tasklet  | 中断   | 同类型不能同时运行 |
| 工作队列 | 进程   | 没有               |

进程上下文和下半部共享数据时加锁, 禁止下半部
中断上下文和下半部共享数据时加锁，禁中断

local_bh_disable禁止软中断和tasklet，并可嵌套调用，实现方式就是preempt_count + SOFTIRQ_OFFSET
#+begin_src c
/*
 * We put the hardirq and softirq counter into the preemption
 * counter. The bitmask has the following meaning:
 *
 * - bits 0-7 are the preemption count (max preemption depth: 256)
 * - bits 8-15 are the softirq count (max # of softirqs: 256)
 *
 * The hardirq count can in theory reach the same as NR_IRQS.
 * In reality, the number of nested IRQS is limited to the stack
 * size as well. For archs with over 1000 IRQS it is not practical
 * to expect that they will all nest. We give a max of 10 bits for
 * hardirq nesting. An arch may choose to give less than 10 bits.
 * m68k expects it to be 8.
 *
 * - bits 16-25 are the hardirq count (max # of nested hardirqs: 1024)
 * - bit 26 is the NMI_MASK
 * - bit 28 is the PREEMPT_ACTIVE flag
 *
 * PREEMPT_MASK: 0x000000ff
 * SOFTIRQ_MASK: 0x0000ff00
 * HARDIRQ_MASK: 0x03ff0000
 *     NMI_MASK: 0x04000000
 */
#+end_src

* 第九章 内核同步介绍

临界区   - 访问和操作共享数据的代码段
竞争条件 - 临界区被同时执行
同步     - 避免并发和防止竞争条件

并发的场景
- 中断，在任何时间异步发生
- 软中断和tasklet, 在任何时间异步发生
- 内核抢占, 内核任务可能被另一任务抢占
- 睡眠及用户空间的同步
- 多处理器

竞争条件
- 和中断处理程序共享资源
- 访问共享资源可被抢占
- 临界区里睡眠
- 多处理器访问同一个共享数据

安全代码
- interrupt-safe
- smp-safe
- preempt-safe

当分析数据是否是共享时
*这个数据是不是全局的？除了当前线程外，其他线程能不能访问它*
*这个数据会不会在进程上下文和中断上下文中共享？它是不是要在两个不同的中断处理程序中共享?*
*进程在访问数据时可不可能被抢占？被调度的新程序会不会访问同一数据*
*这个函数又在另一个处理器上被调度会发生什么*

对于死锁，加锁的顺序所有流程应一致，解锁的顺序无所谓，但最好以获得锁的相反顺序

* 第十章 内核同步方法

- *原子操作*

常用作计数器
#+begin_src c
typedef struct {
	volatile int counter;
} atomic_t;

//静态初始化
#define ATOMIC_INIT(i)	{ (i) }

//原子设值
static inline void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
}

//原子加
static inline void atomic_add(int i, atomic_t *v)
{
	asm volatile(LOCK_PREFIX "addl %1,%0"
		     : "+m" (v->counter)
		     : "ir" (i));
}

//原子减
static inline void atomic_sub(int i, atomic_t *v)
{
	asm volatile(LOCK_PREFIX "subl %1,%0"
		     : "+m" (v->counter)
		     : "ir" (i));
}

//原子自增
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
		     : "+m" (v->counter));
}

//原子自减
static inline void atomic_dec(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "decl %0"
		     : "+m" (v->counter));
}

//原子转换成int
static inline int atomic_read(const atomic_t *v)
{
	return v->counter;
}

//原子减，结果为0返回真
static inline int atomic_sub_and_test(int i, atomic_t *v)
{
	unsigned char c;

	asm volatile(LOCK_PREFIX "subl %2,%0; sete %1"
		     : "+m" (v->counter), "=qm" (c)
		     : "ir" (i) : "memory");
	return c;
}

//原子加，结果是负数返回真
static inline int atomic_add_negative(int i, atomic_t *v)
{
	unsigned char c;

	asm volatile(LOCK_PREFIX "addl %2,%0; sets %1"
		     : "+m" (v->counter), "=qm" (c)
		     : "ir" (i) : "memory");
	return c;
}

//原子加，返回结果
static inline int atomic_add_return(int i, atomic_t *v)
{
	int __i;
#ifdef CONFIG_M386
	unsigned long flags;
	if (unlikely(boot_cpu_data.x86 <= 3))
		goto no_xadd;
#endif
	/* Modern 486+ processor */
	__i = i;
	asm volatile(LOCK_PREFIX "xaddl %0, %1"
		     : "+r" (i), "+m" (v->counter)
		     : : "memory");
	return i + __i;

#ifdef CONFIG_M386
no_xadd: /* Legacy 386 processor */
	raw_local_irq_save(flags);
	__i = atomic_read(v);
	atomic_set(v, i + __i);
	raw_local_irq_restore(flags);
	return i + __i;
#endif
}
#+end_src

*要区分原子性与顺序性*
原子性 - 保证指令全部执行完，要么不执行 例: 两个操作写，结果必为两个写中的一个
顺序性 - 保证多指令出现在不同的线程中也要保证原来的读写顺序 例: 读必须在写之前完成

64位的原子类型是atomic64_t

*原子* 位操作bitops.h，基本单位32位
| Name                          | Description                     |
|-------------------------------+---------------------------------|
| set_bit(nr, addr)             | addr的第nr位设bit1              |
| clear_bit(nr, addr)           | addr的第nr位设bit0              |
| change_bit(nr, addr)          | 翻转addr的第nr位                |
| test_and_set_bit(nr, addr)    | addr的第nr位设bit1 同时返回旧值 |
| test_and_clear_bit(nr, addr)  | addr的第nr位设bit0 同时返回旧值 |
| test_and_change_bit(nr, addr) | 翻转addr的第nr位，同时返回旧值  |
| test_bit(nr, addr)            | 返回第nr位                          |
32位机，31位是最高位，32是下个字的最低位
非原子操作函数名类似，前面加__，如非原子set bit是__set_bit

位搜索

| Name                | Description        |
|---------------------+--------------------|
| find_first_bit      | 第一个bit1         |
| find_first_zero_bit | 第一个bit0         |
| _ffs                | 一个字内第一个bit1 |
| ffz                 | 一个字内第一个bit0 |

- 自旋锁spinlock
自旋锁最多只能被一个执行线程持有，如果已被持有，则一直进行 *忙循环*
自旋锁不可递归，同时在单处理器中，自旋锁只是禁止内核抢占，如果编译内核时不指定内核抢占，那么什么都不做

常用的加锁代码:
#+begin_src c
DEFINE_SPINLOCK(mr_lock);
spin_lock(&mr_lock);
//临界区
spin_unlock(&mr_lock);
#+end_src

中断程序使用自旋锁:
#+begin_src c
DEFINE_SPINLOCK(mr_lock);
unsigned log flags;

spin_lock_irqsave(&mr_lock, flags); //直接关中断可以spin_lock_irq，但是不推荐使用
//临界区
spin_lock_irqrestore(&mr_lock, flags);
#+end_src

| spin_lock              | 获取锁                             |
| spin_lock_irq          | 禁止本地中断并获取锁               |
| spin_lock_irqsave      | 保存中断状态，禁止本地中断并获取锁 |
| spin_unlock            | 释放锁                             |
| spin_unlock_irq        | 释放锁并激活中断                   |
| spin_unlock_irqrestore | 释放锁，恢复中断状态               |
| spin_lock_init         | 动态初始化                         |
| spin_trylock           | 试图获取锁，如未成功返回非0        |
| spin_is_locked         | 锁被获取返回非0                    |
| spin_lock_bh           | 获取锁并禁止下半部                 |
| spin_unlock_bh         | 释放锁并激活下半部                 |

读写自旋锁类似，区别是可允许多个读

- 信号量 semaphore.h
是一种 *睡眠* 锁，适用于锁会被长时间持有的情况， *只能在进程上下文中用*
只允许一个使用锁时为二值信号量

| sema_init          | 以指定的计数值初始化                     |
| init_MUTEX         | 动态初始化二值信号量                     |
| init_MUTEX_LOCKED  | 动态初始化已加锁二值信号量               |
| down_interruptible | 获得锁，会睡眠，会被信号打断会返回-ENITR |
| down               | 获得锁，会睡眠，不会被信号打断           |
| down_trylock       | 试图获得锁，不成功返回非0                |
| up                 | 释放                                     |

读写信号量类似，区别是可允许多个读，函数返回值会有所区别

内核有单独的互斥锁，代替二值信号量

| mutex_init    | 初始化                    |
| mutex_lock    | 加锁，会睡眠              |
| mutex_unlock  | 解锁                      |
| mutex_trylock | 试图获得锁，如成功则返回1 |
| mutex_is_lock | 锁已被用返回1             |
#+begin_src c
/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
#+end_src

| 需求              | 加锁方法 |
|-------------------+----------|
| 低开销加锁        | 自旋锁   |
| 短期锁定          | 自旋锁   |
| 长期加锁          | 互斥锁   |
| 中断上下文&下半部 | 自旋锁   |
| 需要睡眠          | 互斥锁   |

- 完成变量（条件变量）completion.h
| init_completion     | 初始化                     |
| wait_for_completion | 等待指定的完成变量接收信号 |
| complete            | 发信号唤醒任何等待任务     |

- 顺序锁 seqlock.h
对写者有利，有读时写锁也可获得，读时会在前后读计数确定中间没有写操作
jiffies用的是顺序锁

#+begin_src c
//例程

seqlock_t mr_seq_lock = DEFINE_SEQLOCK(mr_seq_lock);

//写
write_seqlock(&mr_seq_lock);
//do sth writing
write_sequnlock(&mr_seq_lock);


//读
unsigned long seq;
do {
    seq = read_seqbegin(&mr_seq_lock);
    //do sth reading
} while(read_seqretry(&mr_seq_lock, seq));

#+end_src

- 禁止抢占
常用于保护per处理器数据，不需要用自旋锁，所以关内核抢占就行了
preempt_disable和preempt_enable

- 顺序和屏障
多用于硬件驱动程序

编译器 - 对于看起来不相关的的多个读写，优化打乱顺序
处理器 - 运行时看起来不相关的的多个读写，打乱顺序

| 屏障                     | 描述                                   |
|--------------------------+----------------------------------------|
| rmb                      | 阻止屏障前后的读发生重排序             |
| read_barrier_depends     | 阻止屏障前后的有数据依赖的读发生重排序 |
| wmb                      | 阻止屏障前后的写发生重排序             |
| mb                       | 阻止屏障前后的读写发生重排序           |
| smp_rmb                  | 支持SMP                                |
| smp_read_barrier_depends | 支持SMP                                |
| smp_wmb                  | 支持SMP                                |
| smp_mb                   | 支持SMP                                |
| barrier                  | 阻止编译器优化前后顺序                 |
处理器屏障自然有编译器屏障
*其实intel x86芯片不会执行乱序存储，所以wmb相当于空*


* 第十一章 定时器和时间管理

*__KERNEL__* 在编译内核时被定义，有时候用户程序需要内核头文件，这时候__KERNEL__就不会被定义
如下，内核中HZ为CONFIG_HZ,在.config文件中，可通过make menuconfig配置，这里为1000，也就是说每秒钟时钟中断1000次
#+begin_src c
#ifndef __ASM_GENERIC_PARAM_H
#define __ASM_GENERIC_PARAM_H

// When you compile your kernel, __KERNEL__ is defined on the command line.

// User-space programs need access to the kernel headers, but some of the info in kernel headers is intended only for the kernel. Wrapping some statements in an #ifdef __KERNEL__/#endif block ensures that user-space programs don't see those statements.
#ifdef __KERNEL__ //如果
# define HZ		CONFIG_HZ	/* Internal kernel timer frequency */
# define USER_HZ	100		/* some user interfaces are */
# define CLOCKS_PER_SEC	(USER_HZ)       /* in "ticks" like times() */
#endif

#ifndef HZ
#define HZ 100 //用户程序永远是100，内核返回用户时会用USER_HZ转换jiffies_to_clock_t
#endif

#ifndef EXEC_PAGESIZE
#define EXEC_PAGESIZE	4096
#endif

#ifndef NOGROUP
#define NOGROUP		(-1)
#endif

#define MAXHOSTNAMELEN	64	/* max length of hostname */

#endif /* __ASM_GENERIC_PARAM_H */
#+end_src

#+begin_src txt
//.config文件
# CONFIG_CC_STACKPROTECTOR is not set
# CONFIG_HZ_100 is not set
# CONFIG_HZ_250 is not set
# CONFIG_HZ_300 is not set
CONFIG_HZ_1000=y
CONFIG_HZ=1000
#+end_src

jiffies是两次连续的时钟节拍之间的时间,jiffies_64是jiffies同一个地址的64位整数
*jiffies和jiffies_64通过链接脚本配置为同一个地址*
#+begin_src c
//

/*
 *	These inlines deal with timer wrapping correctly. You are 
 *	strongly encouraged to use them
 *	1. Because people otherwise forget
 *	2. Because if the timer wrap changes in future you won't have to
 *	   alter your driver code.
 *
 * time_after(a,b) returns true if the time a is after time b.
 *
 * Do this with "<0" and ">=0" to only test the sign of the result. A
 * good compiler would generate better code (and a really good compiler
 * wouldn't care). Gcc is currently neither.
 */
#define time_after(a,b)		\
	(typecheck(unsigned long, a) && \
	 typecheck(unsigned long, b) && \
	 ((long)(b) - (long)(a) < 0))
#define time_before(a,b)	time_after(b,a)
#define time_after_eq(a,b)	\
	(typecheck(unsigned long, a) && \
	 typecheck(unsigned long, b) && \
	 ((long)(a) - (long)(b) >= 0))
#define time_before_eq(a,b)	time_after_eq(b,a)
#+end_src
用有符号减来避免wrap around(溢出后归0)的问题，但是有个隐含的点，时间间隔不能太长(1/2个unsigned long)，当然实际也不会发生
#+begin_src c
#include <stdio.h>

#define time_after(a,b) \
	 ((int)(b) - (int)(a) < 0)

int main(int argc, char *argv[])
{
    unsigned int a = 0x100;
    unsigned int b = 0x9;

    printf("%d %d %d\n", a, b, time_after(a, b));

    a = 0x0;
    b = 0xffffffff;
    printf("%d %d %d\n", a, b, time_after(a, b));

    a = 0x80000000;
    b = 0x7fffffff;
    printf("%d %d %d\n", a, b, time_after(a, b));
    return 0;
}
#+end_src

- 系统定时器
就是上面讨论的，一种周期性触发中断机制，X86体系中用可编程中断时钟PIT，基于晶振

- 实时时钟
RTC是另外一个时钟设备，其在系统关闭后，仍可以靠电池保持计时，用于初始化xtime
#+begin_src c
static void tick_periodic(int cpu)
{
	if (tick_do_timer_cpu == cpu) {
		write_seqlock(&xtime_lock);

		/* Keep track of the next tick event */
		tick_next_period = ktime_add(tick_next_period, tick_period);

		do_timer(1); //自增jiffies
		write_sequnlock(&xtime_lock);
	}

	update_process_times(user_mode(get_irq_regs())); //更新时间,标记TIMER软中断,减少当前进程时间片，设置need_resched
	profile_tick(CPU_PROFILING);
}
#+end_src

- 定时器
#+begin_src c
struct timer_list {
	struct list_head entry;
	unsigned long expires;

	void (*function)(unsigned long);
	unsigned long data;

	struct tvec_base *base;
#ifdef CONFIG_TIMER_STATS
	void *start_site;
	char start_comm[16];
	int start_pid;
#endif
#ifdef CONFIG_LOCKDEP
	struct lockdep_map lockdep_map;
#endif
};

struct timer_list my_timer; //定义

init_timer(&my_timer); //初始化
my_timer.expires = jiffies + delay; //超时节拍
my_timer.data = 0;
my_timer.function = myfunction; //超时回调

add_timer(&my_timer); //加定时器
mod_timer(&my_timer, jiffies+new_delay); //修改新的定时
del_timer(&my_timer); //删除定时器,SMP可能停止同时在运行超时函数
del_timer_sync(&timer); //等待可能在运行的超时回调后再删除定时，会睡眠
#+end_src

定时器在下半部中执行，为了提高效率，内核将定时器超时时间划分为五组并随组下移

- 延时
忙等，会浪费CPU
#+begin_src c
unsigned long delay = jiffies + 2 * HZ;
while(time_before(jiffies, delay));
#+end_src

调度等，不能在中断上下文中使用，实际上不该在中断中延时
#+begin_src c
unsigned long delay = jiffies + 2 * HZ;
while(time_before(jiffies, delay))
    cond_resched();
#+end_src

短延时
实现方式是控制忙循环次数，因为时钟节拍精度不够
#+begin_src c
/* 0x10c7 is 2**32 / 1000000 (rounded up) */
#define udelay(n) (__builtin_constant_p(n) ? \
	((n) > 20000 ? __bad_udelay() : __const_udelay((n) * 0x10c7ul)) : \
	__udelay(n))

/* 0x5 is 2**32 / 1000000000 (rounded up) */
#define ndelay(n) (__builtin_constant_p(n) ? \
	((n) > 20000 ? __bad_ndelay() : __const_udelay((n) * 5ul)) : \
	__ndelay(n))

#define mdelay(n) (\
	(__builtin_constant_p(n) && (n)<=MAX_UDELAY_MS) ? udelay((n)*1000) : \
	({unsigned long __ms=(n); while (__ms--) udelay(1000);}))
#endif
#+end_src

schedule_timeout 延迟timeout再运行，延时中会睡眠
通过定时器和调度器实现，所以精通为1/HZ
#+begin_src c
//设为可中断,不想接受信号也可以设为TASK_UNINTERRUPTIBLE
set_current_state(TASK_INTERRUPTIBLE);
schedule_timeout(timeout);
#+end_src

* 第十二章 内存管理
32位一般用4K的页，64位一般用8K的页

- 物理页结构struct page
#+begin_src c
struct page {
	unsigned long flags;           //页状态
	atomic_t _count;               //引用计数
    atomic_t _mapcount;            //
    unsigned long private;
    struct address_space *mapping;
    pgoff_t index;
	struct list_head lru;
	void *virtual;                 //虚拟地址
};
#+end_src

- 区
内核把页划分为不同的区，区的实际使用和体系结构相关
*像ISA设备DMA内存有限制，比如只能用24位，所以0-16MB预留*
 | 区           | 描述           | 物理内存   |
 |--------------+----------------+------------|
 | ZONE_DMA     | DMA            | < 16MB     |
 | ZONE_NORMAL  | 正常可寻址的页 | 16 ~ 896MB |
 | ZONE_HIGHMEM | 动态映射的页   | > 896MB    |
像x64架构，并没有ZONE_HIGHMEM区
#+begin_src c
struct zone {
	/* Fields commonly accessed by the page allocator */

	/* zone watermarks, access with *_wmark_pages(zone) macros */
	unsigned long watermark[NR_WMARK];

	/*
	 * We don't know if the memory that we're going to allocate will be freeable
	 * or/and it will be released eventually, so to avoid totally wasting several
	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
	 * to run OOM on the lower zones despite there's tons of freeable ram
	 * on the higher zones). This array is recalculated at runtime if the
	 * sysctl_lowmem_reserve_ratio sysctl changes.
	 */
	unsigned long		lowmem_reserve[MAX_NR_ZONES];

#ifdef CONFIG_NUMA
	int node;
	/*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
#endif
	struct per_cpu_pageset __percpu *pageset;
	/*
	 * free areas of different sizes
	 */
	spinlock_t		lock;
	int                     all_unreclaimable; /* All pages pinned */
#ifdef CONFIG_MEMORY_HOTPLUG
	/* see spanned/present_pages for more description */
	seqlock_t		span_seqlock;
#endif
	struct free_area	free_area[MAX_ORDER];

#ifndef CONFIG_SPARSEMEM
	/*
	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
	 * In SPARSEMEM, this map is stored in struct mem_section
	 */
	unsigned long		*pageblock_flags;
#endif /* CONFIG_SPARSEMEM */


	ZONE_PADDING(_pad1_)

	/* Fields commonly accessed by the page reclaim scanner */
	spinlock_t		lru_lock;	
	struct zone_lru {
		struct list_head list;
	} lru[NR_LRU_LISTS];

	struct zone_reclaim_stat reclaim_stat;

	unsigned long		pages_scanned;	   /* since last reclaim */
	unsigned long		flags;		   /* zone flags, see below */

	/* Zone statistics */
	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];

	/*
	 * prev_priority holds the scanning priority for this zone.  It is
	 * defined as the scanning priority at which we achieved our reclaim
	 * target at the previous try_to_free_pages() or balance_pgdat()
	 * invocation.
	 *
	 * We use prev_priority as a measure of how much stress page reclaim is
	 * under - it drives the swappiness decision: whether to unmap mapped
	 * pages.
	 *
	 * Access to both this field is quite racy even on uniprocessor.  But
	 * it is expected to average out OK.
	 */
	int prev_priority;

	/*
	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	 * this zone's LRU.  Maintained by the pageout code.
	 */
	unsigned int inactive_ratio;


	ZONE_PADDING(_pad2_)
	/* Rarely used or read-mostly fields */

	/*
	 * wait_table		-- the array holding the hash table
	 * wait_table_hash_nr_entries	-- the size of the hash table array
	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
	wait_queue_head_t	* wait_table;
	unsigned long		wait_table_hash_nr_entries;
	unsigned long		wait_table_bits;

	/*
	 * Discontig memory support fields.
	 */
	struct pglist_data	*zone_pgdat;
	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
	unsigned long		zone_start_pfn;

	/*
	 * zone_start_pfn, spanned_pages and present_pages are all
	 * protected by span_seqlock.  It is a seqlock because it has
	 * to be read outside of zone->lock, and it is done in the main
	 * allocator path.  But, it is written quite infrequently.
	 *
	 * The lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 */
	unsigned long		spanned_pages;	/* total size, including holes */
	unsigned long		present_pages;	/* amount of memory (excluding holes) */

	/*
	 * rarely used fields:
	 */
	const char		*name;
} ____cacheline_internodealigned_in_smp;
#+end_src

| Name             | Description                                     |
|------------------+-------------------------------------------------|
| alloc_page       | 只分配一页                                      |
| alloc_pages      | 分配2^order个页                                 |
| __get_free_page  | 只分配一页,返回指向逻辑地址的指针               |
| __get_free_pages | 分配2^order个页,返回指向逻辑地址的指针          |
| get_zeroed_page  | 只分配一页,让其内容填充0,返回指向逻辑地址的指针 |
| __free_pages     | 释放页                                          |
| free_pages       | 释放页,入参是逻辑地址                           |
| free_page        | 释放一个页,入参是逻辑地址                       |

- kmalloc分配以字节为单位,kfree是对应的释放程序

GFP标记位
| Name                   | Description         |
|------------------------+---------------------|
| 进程上下文，可以睡眠   | GFP_KERNEL          |
| 进程上下文，不可以睡眠 | GFP_ATOMIC          |
| 中断处理程序           | GFP_ATOMIC          |
| 软中断                 | GFP_ATOMIC          |
| tasklet                | GFP_ATOMIC          |
| DMA，可以睡眠          | GFP_DMA和GFP_KERNEL |
| DMA, 不可以睡眠        | GFP_DMA和GFP_ATOMIC |

vmalloc和kmalloc类似，但不保证物理页上的连续，不过为保持性能，常用kmalloc
同时vmalloc可以睡眠

- slab分配器,为了提高申请释放内存的性能,kmalloc基于slab实现,管理slab的是kmem_cache
*kmalloc有kmem_cache，每个kmem_cache管理一个固定大小,每个kmem_cache有多个slab*

#+begin_src c
//创建高速缓存，可以睡眠
struct kmem_cache *
kmem_cache_create (const char *name, //名字
    size_t size,      //每个元素的大小
    size_t align,     //对齐
	unsigned long flags,
    void (*ctor)(void *))

//删除高速缓存
void kmem_cache_destroy(struct kmem_cache *cachep)
#+end_src

默认32位的内核栈大小8K，64位的内核栈大小16K

高端内存的页可被映射到3G~4G,分为永久映射kmap和临时映射kmap_atomic(系统保留了一些虚拟地址),对应free的操作是kunmap和kunmap_atomic

Linux 高端内存是针对物理内存来说的，虚拟内存没有高端这个概念。Linux 系统将虚拟内存分为两个部分，即用户地址空间和内核地址空间，对于 32 位系统来说，虚拟地址空间为 4GB，其中用户空间范围为 0-3GB，内核空间范围为 3-4GB。Linux 将 3GB 开始的内核虚拟地址空间的 896M 地址直接映射到物理地址空间的 0-896M，这部分是永久性映射，剩下的 128M 则可根据需要进行动态映射，也称临时性映射。如果没有动态映射，那么 1GB 的内核虚拟地址空间最多只能访问 1GB 的物理内存，那么如果物理内存大于 1GB，就会有一部分物理内存无法被内核访问到，显然不是我们想要的。因此，对于 32 位系统来说，高端物理内存是内核虚拟地址空间后 128M 可动态映射使内核得以访问的物理内存，用户空间虚拟地址也是映射到这部分物理内存

- percpu变量
#+begin_src c
//静态
DEFINE_PER_CPU(type, name);
DECLARE_PER_CPU(type, name);

//禁止内核抢占,取值
#define get_cpu_var(var) (*({				\
	preempt_disable();				\
	&__get_cpu_var(var); }))

//使能内核抢占
#define put_cpu_var(var) do {				\
	(void)&(var);					\
	preempt_enable();				\
} while (0)

//无锁
#define per_cpu_ptr(ptr, cpu)	SHIFT_PERCPU_PTR((ptr), per_cpu_offset((cpu)))


//动态申请
#define alloc_percpu(type)	\
	(typeof(type) __percpu *)__alloc_percpu(sizeof(type), __alignof__(type))

extern void __percpu *__alloc_percpu(size_t size, size_t align);
extern void free_percpu(void __percpu *__pdata);
#+end_src

Sample
#+begin_src c
void *percpu_ptr;
unsigned long *foo;

percpu_ptr = alloc_percpu(unsigned long);
...
foo = get_cpu_var(percpu_ptr);
...
put_cpu_var(percpu_ptr);
#+end_src

* 第十三章 虚拟文件系统
VFS屏蔽了不同文件系统，抽象出文件,目录项,索引节点和安装点

*文件的相关信息和文件本身和不同的概念，文件相关信息，被称作文件的元数据，被存储在一个单独的数据结构中，称为inode*

- 超级块对象结构struct super_block，包含最重要的struct super_operations，里面有各项回调函数
超级块用于存储特定文件系统的信息，通常对应于存放在磁盘特定扇区中的文件系统超级块或文件系统控制块
#+begin_src c
struct super_block {
	struct list_head	s_list;		/* Keep this first */
	dev_t			s_dev;		/* search index; _not_ kdev_t */
	unsigned char		s_dirt;
	unsigned char		s_blocksize_bits;
	unsigned long		s_blocksize;
	loff_t			s_maxbytes;	/* Max file size */
	struct file_system_type	*s_type;
	const struct super_operations	*s_op;
	const struct dquot_operations	*dq_op;
	const struct quotactl_ops	*s_qcop;
	const struct export_operations *s_export_op;
	unsigned long		s_flags;
	unsigned long		s_magic;
	struct dentry		*s_root;
	struct rw_semaphore	s_umount;
	struct mutex		s_lock;
	int			s_count;
	int			s_need_sync;
	atomic_t		s_active;
#ifdef CONFIG_SECURITY
	void                    *s_security;
#endif
	struct xattr_handler	**s_xattr;

	struct list_head	s_inodes;	/* all inodes */
	struct hlist_head	s_anon;		/* anonymous dentries for (nfs) exporting */
	struct list_head	s_files;
	/* s_dentry_lru and s_nr_dentry_unused are protected by dcache_lock */
	struct list_head	s_dentry_lru;	/* unused dentry lru */
	int			s_nr_dentry_unused;	/* # of dentry on lru */

	struct block_device	*s_bdev;
	struct backing_dev_info *s_bdi;
	struct mtd_info		*s_mtd;
	struct list_head	s_instances;
	struct quota_info	s_dquot;	/* Diskquota specific options */

	int			s_frozen;
	wait_queue_head_t	s_wait_unfrozen;

	char s_id[32];				/* Informational name */

	void 			*s_fs_info;	/* Filesystem private info */
	fmode_t			s_mode;

	/* Granularity of c/m/atime in ns.
	   Cannot be worse than a second */
	u32		   s_time_gran;

	/*
	 * The next field is for VFS *only*. No filesystems have any business
	 * even looking at it. You had been warned.
	 */
	struct mutex s_vfs_rename_mutex;	/* Kludge */

	/*
	 * Filesystem subtype.  If non-empty the filesystem type field
	 * in /proc/mounts will be "type.subtype"
	 */
	char *s_subtype;

	/*
	 * Saved mount options for lazy filesystems using
	 * generic_show_options()
	 */
	char *s_options;
};

struct super_operations {
   	struct inode *(*alloc_inode)(struct super_block *sb);
	void (*destroy_inode)(struct inode *);

   	void (*dirty_inode) (struct inode *);
	int (*write_inode) (struct inode *, struct writeback_control *wbc);
	void (*drop_inode) (struct inode *);
	void (*delete_inode) (struct inode *);
	void (*put_super) (struct super_block *);
	void (*write_super) (struct super_block *);
	int (*sync_fs)(struct super_block *sb, int wait);
	int (*freeze_fs) (struct super_block *);
	int (*unfreeze_fs) (struct super_block *);
	int (*statfs) (struct dentry *, struct kstatfs *);
	int (*remount_fs) (struct super_block *, int *, char *);
	void (*clear_inode) (struct inode *);
	void (*umount_begin) (struct super_block *);

	int (*show_options)(struct seq_file *, struct vfsmount *);
	int (*show_stats)(struct seq_file *, struct vfsmount *);
#ifdef CONFIG_QUOTA
	ssize_t (*quota_read)(struct super_block *, int, char *, size_t, loff_t);
	ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);
#endif
	int (*bdev_try_to_free_page)(struct super_block*, struct page*, gfp_t);
};
#+end_src

- 索引节点对象inode,包含内核在操作文件或目录时需要的全部信息
一个inode代表文件系统中的一个文件，也可代表特殊文件，如设备文件或者管道文件，包含struct inode_operations
#+begin_src c
struct inode {
	struct hlist_node	i_hash;
	struct list_head	i_list;		/* backing dev IO list */
	struct list_head	i_sb_list;
	struct list_head	i_dentry;
	unsigned long		i_ino;
	atomic_t		i_count;
	unsigned int		i_nlink;
	uid_t			i_uid;
	gid_t			i_gid;
	dev_t			i_rdev;
	unsigned int		i_blkbits;
	u64			i_version;
	loff_t			i_size;
#ifdef __NEED_I_SIZE_ORDERED
	seqcount_t		i_size_seqcount;
#endif
	struct timespec		i_atime;
	struct timespec		i_mtime;
	struct timespec		i_ctime;
	blkcnt_t		i_blocks;
	unsigned short          i_bytes;
	umode_t			i_mode;
	spinlock_t		i_lock;	/* i_blocks, i_bytes, maybe i_size */
	struct mutex		i_mutex;
	struct rw_semaphore	i_alloc_sem;
	const struct inode_operations	*i_op;
	const struct file_operations	*i_fop;	/* former ->i_op->default_file_ops */
	struct super_block	*i_sb;
	struct file_lock	*i_flock;
	struct address_space	*i_mapping;
	struct address_space	i_data;
#ifdef CONFIG_QUOTA
	struct dquot		*i_dquot[MAXQUOTAS];
#endif
	struct list_head	i_devices;
	union {
		struct pipe_inode_info	*i_pipe;
		struct block_device	*i_bdev;
		struct cdev		*i_cdev;
	};

	__u32			i_generation;

#ifdef CONFIG_FSNOTIFY
	__u32			i_fsnotify_mask; /* all events this inode cares about */
	struct hlist_head	i_fsnotify_mark_entries; /* fsnotify mark entries */
#endif

#ifdef CONFIG_INOTIFY
	struct list_head	inotify_watches; /* watches on this inode */
	struct mutex		inotify_mutex;	/* protects the watches list */
#endif

	unsigned long		i_state;
	unsigned long		dirtied_when;	/* jiffies of first dirtying */

	unsigned int		i_flags;

	atomic_t		i_writecount;
#ifdef CONFIG_SECURITY
	void			*i_security;
#endif
#ifdef CONFIG_FS_POSIX_ACL
	struct posix_acl	*i_acl;
	struct posix_acl	*i_default_acl;
#endif
	void			*i_private; /* fs or device private pointer */
};

struct inode_operations {
	int (*create) (struct inode *,struct dentry *,int, struct nameidata *);
	struct dentry * (*lookup) (struct inode *,struct dentry *, struct nameidata *);
	int (*link) (struct dentry *,struct inode *,struct dentry *);
	int (*unlink) (struct inode *,struct dentry *);
	int (*symlink) (struct inode *,struct dentry *,const char *);
	int (*mkdir) (struct inode *,struct dentry *,int);
	int (*rmdir) (struct inode *,struct dentry *);
	int (*mknod) (struct inode *,struct dentry *,int,dev_t);
	int (*rename) (struct inode *, struct dentry *,
			struct inode *, struct dentry *);
	int (*readlink) (struct dentry *, char __user *,int);
	void * (*follow_link) (struct dentry *, struct nameidata *);
	void (*put_link) (struct dentry *, struct nameidata *, void *);
	void (*truncate) (struct inode *);
	int (*permission) (struct inode *, int);
	int (*check_acl)(struct inode *, int);
	int (*setattr) (struct dentry *, struct iattr *);
	int (*getattr) (struct vfsmount *mnt, struct dentry *, struct kstat *);
	int (*setxattr) (struct dentry *, const char *,const void *,size_t,int);
	ssize_t (*getxattr) (struct dentry *, const char *, void *, size_t);
	ssize_t (*listxattr) (struct dentry *, char *, size_t);
	int (*removexattr) (struct dentry *, const char *);
	void (*truncate_range)(struct inode *, loff_t, loff_t);
	long (*fallocate)(struct inode *inode, int mode, loff_t offset,
			  loff_t len);
	int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start,
		      u64 len);
};
#+end_src

- 目录项对象
'/mnt/cdrom/foo'中 / mnt cdrom foo都属于目录项对象，VFS在执行目录操作时，会现场创建目录项对象
与超级块和索引节点不同，目录项不会存到磁盘里面
#+begin_src c
struct dentry {
	atomic_t d_count;
	unsigned int d_flags;		/* protected by d_lock */
	spinlock_t d_lock;		/* per dentry lock */
	int d_mounted;
	struct inode *d_inode;		/* Where the name belongs to - NULL is
					 * negative */
	/*
	 * The next three fields are touched by __d_lookup.  Place them here
	 * so they all fit in a cache line.
	 */
	struct hlist_node d_hash;	/* lookup hash list */
	struct dentry *d_parent;	/* parent directory */
	struct qstr d_name;

	struct list_head d_lru;		/* LRU list */
	/*
	 * d_child and d_rcu can share memory
	 */
	union {
		struct list_head d_child;	/* child of parent list */
	 	struct rcu_head d_rcu;
	} d_u;
	struct list_head d_subdirs;	/* our children */
	struct list_head d_alias;	/* inode alias list */
	unsigned long d_time;		/* used by d_revalidate */
	const struct dentry_operations *d_op;
	struct super_block *d_sb;	/* The root of the dentry tree */
	void *d_fsdata;			/* fs-specific data */

	unsigned char d_iname[DNAME_INLINE_LEN_MIN];	/* small names */
};

struct dentry_operations {
	int (*d_revalidate)(struct dentry *, struct nameidata *);
	int (*d_hash) (struct dentry *, struct qstr *);
	int (*d_compare) (struct dentry *, struct qstr *, struct qstr *);
	int (*d_delete)(struct dentry *);
	void (*d_release)(struct dentry *);
	void (*d_iput)(struct dentry *, struct inode *);
	char *(*d_dname)(struct dentry *, char *, int);
};
#+end_src

- 文件对象
文件对象是已打开的文件在内存中的表示，和目录项一样没有对应的磁盘数据
#+begin_src c
struct file {
	/*
	 * fu_list becomes invalid after file_free is called and queued via
	 * fu_rcuhead for RCU freeing
	 */
	union {
		struct list_head	fu_list;
		struct rcu_head 	fu_rcuhead;
	} f_u;
	struct path		f_path;
#define f_dentry	f_path.dentry
#define f_vfsmnt	f_path.mnt
	const struct file_operations	*f_op;
	spinlock_t		f_lock;  /* f_ep_links, f_flags, no IRQ */
	atomic_long_t		f_count;
	unsigned int 		f_flags;
	fmode_t			f_mode;
	loff_t			f_pos;
	struct fown_struct	f_owner;
	const struct cred	*f_cred;
	struct file_ra_state	f_ra;

	u64			f_version;
#ifdef CONFIG_SECURITY
	void			*f_security;
#endif
	/* needed for tty driver, and maybe others */
	void			*private_data;

#ifdef CONFIG_EPOLL
	/* Used by fs/eventpoll.c to link all the hooks to this file */
	struct list_head	f_ep_links;
#endif /* #ifdef CONFIG_EPOLL */
	struct address_space	*f_mapping;
#ifdef CONFIG_DEBUG_WRITECOUNT
	unsigned long f_mnt_write_state;
#endif
};

struct file_operations {
	struct module *owner;
	loff_t (*llseek) (struct file *, loff_t, int);
	ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
	ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
	ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t);
	ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t);
	int (*readdir) (struct file *, void *, filldir_t);
	unsigned int (*poll) (struct file *, struct poll_table_struct *);
	int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long);
	long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
	long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
	int (*mmap) (struct file *, struct vm_area_struct *);
	int (*open) (struct inode *, struct file *);
	int (*flush) (struct file *, fl_owner_t id);
	int (*release) (struct inode *, struct file *);
	int (*fsync) (struct file *, struct dentry *, int datasync);
	int (*aio_fsync) (struct kiocb *, int datasync);
	int (*fasync) (int, struct file *, int);
	int (*lock) (struct file *, int, struct file_lock *);
	ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int);
	unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
	int (*check_flags)(int);
	int (*flock) (struct file *, int, struct file_lock *);
	ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);
	ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);
	int (*setlease)(struct file *, long, struct file_lock **);
};
#+end_src

- 根结构

file_system_type - 描述各种特定文件系统类型
vfsmount         - 安装节点
#+begin_src c
struct file_system_type {
	const char *name;
	int fs_flags;
	int (*get_sb) (struct file_system_type *, int,
		       const char *, void *, struct vfsmount *);
	void (*kill_sb) (struct super_block *);
	struct module *owner;
	struct file_system_type * next;
	struct list_head fs_supers;

	struct lock_class_key s_lock_key;
	struct lock_class_key s_umount_key;

	struct lock_class_key i_lock_key;
	struct lock_class_key i_mutex_key;
	struct lock_class_key i_mutex_dir_key;
	struct lock_class_key i_alloc_sem_key;
};

struct vfsmount {
	struct list_head mnt_hash;
	struct vfsmount *mnt_parent;	/* fs we are mounted on */
	struct dentry *mnt_mountpoint;	/* dentry of mountpoint */
	struct dentry *mnt_root;	/* root of the mounted tree */
	struct super_block *mnt_sb;	/* pointer to superblock */
	struct list_head mnt_mounts;	/* list of children, anchored here */
	struct list_head mnt_child;	/* and going through their mnt_child */
	int mnt_flags;
	/* 4 bytes hole on 64bits arches */
	const char *mnt_devname;	/* Name of device e.g. /dev/dsk/hda1 */
	struct list_head mnt_list;
	struct list_head mnt_expire;	/* link in fs-specific expiry list */
	struct list_head mnt_share;	/* circular list of shared mounts */
	struct list_head mnt_slave_list;/* list of slave mounts */
	struct list_head mnt_slave;	/* slave list entry */
	struct vfsmount *mnt_master;	/* slave is on master->mnt_slave_list */
	struct mnt_namespace *mnt_ns;	/* containing namespace */
	int mnt_id;			/* mount identifier */
	int mnt_group_id;		/* peer group identifier */
	/*
	 * We put mnt_count & mnt_expiry_mark at the end of struct vfsmount
	 * to let these frequently modified fields in a separate cache line
	 * (so that reads of mnt_flags wont ping-pong on SMP machines)
	 */
	atomic_t mnt_count;
	int mnt_expiry_mark;		/* true if marked for expiry */
	int mnt_pinned;
	int mnt_ghosts;
#ifdef CONFIG_SMP
	int __percpu *mnt_writers;
#else
	int mnt_writers;
#endif
};
#+end_src

- 进程相关
task_struct中的files_struct, 描述打开的文件描述符，通常各进程独立
task_struct中的fs_struct, 描述进程的工作目录和根目录, 通常各进程独立
task_struct中的mnt_namespace, 描述一个命令空间，通常多进程共享
#+begin_src c
struct files_struct {
  /*
   * read mostly part
   */
	atomic_t count;
	struct fdtable *fdt;
	struct fdtable fdtab;
  /*
   * written part on a separate cache line in SMP
   */
	spinlock_t file_lock ____cacheline_aligned_in_smp;
	int next_fd;
	struct embedded_fd_set close_on_exec_init;
	struct embedded_fd_set open_fds_init;
	struct file * fd_array[NR_OPEN_DEFAULT];
};

struct fs_struct {
	int users;
	rwlock_t lock;
	int umask;
	int in_exec;
	struct path root, pwd;
};

struct mnt_namespace {
	atomic_t		count;
	struct vfsmount *	root;
	struct list_head	list;
	wait_queue_head_t poll;
	int event;
};
#+end_src

* 第十四章 块I/O层
块设备   - 随机访问固定大小数据片
字符设备 - 以字符流的方式被访问

缓冲区头定义了缓冲区与一个块的对应关系，buffer_head太重，bio更为轻量
#+begin_src c
struct buffer_head {
	unsigned long b_state;		/* buffer state bitmap (see above) */
	struct buffer_head *b_this_page;/* circular list of page's buffers */
	struct page *b_page;		/* the page this bh is mapped to */

	sector_t b_blocknr;		/* start block number */
	size_t b_size;			/* size of mapping */
	char *b_data;			/* pointer to data within the page */

	struct block_device *b_bdev;
	bh_end_io_t *b_end_io;		/* I/O completion */
 	void *b_private;		/* reserved for b_end_io */
	struct list_head b_assoc_buffers; /* associated with another mapping */
	struct address_space *b_assoc_map;	/* mapping this buffer is
						   associated with */
	atomic_t b_count;		/* users using this buffer_head */
};

struct bio {
	sector_t		bi_sector;	/* device address in 512 byte
						   sectors */
	struct bio		*bi_next;	/* request queue link */
	struct block_device	*bi_bdev;
	unsigned long		bi_flags;	/* status, command, etc */
	unsigned long		bi_rw;		/* bottom bits READ/WRITE,
						 * top bits priority
						 */

	unsigned short		bi_vcnt;	/* how many bio_vec's */
	unsigned short		bi_idx;		/* current index into bvl_vec */

	/* Number of segments in this BIO after
	 * physical address coalescing is performed.
	 */
	unsigned int		bi_phys_segments;

	unsigned int		bi_size;	/* residual I/O count */

	/*
	 * To keep track of the max segment size, we account for the
	 * sizes of the first and last mergeable segments in this bio.
	 */
	unsigned int		bi_seg_front_size;
	unsigned int		bi_seg_back_size;

	unsigned int		bi_max_vecs;	/* max bvl_vecs we can hold */

	unsigned int		bi_comp_cpu;	/* completion CPU */

	atomic_t		bi_cnt;		/* pin count */

	struct bio_vec		*bi_io_vec;	/* the actual vec list */

	bio_end_io_t		*bi_end_io;

	void			*bi_private;
#if defined(CONFIG_BLK_DEV_INTEGRITY)
	struct bio_integrity_payload *bi_integrity;  /* data integrity */
#endif

	bio_destructor_t	*bi_destructor;	/* destructor */

	/*
	 * We can inline a number of vecs at the end of the bio, to avoid
	 * double allocations for a small number of bio_vecs. This member
	 * MUST obviously be kept at the very end of the bio.
	 */
	struct bio_vec		bi_inline_vecs[0];
};
#+end_src
I/O调度程序调度虚拟块设备的多个磁盘请求，以便降低磁盘寻址时间，确保磁盘性能的最优化
I/O调度程序的工作是管理块设备的请求队列，主要工作是对请求队列合并与排序

可以通过elevator=xxx选择不同的调度程序

| 参数     | I/O调度程序    |
|----------+----------------|
| as       | 预测           |
| cfq      | 完全公正的排队 |
| deadline | 最终期限       |
| noop     | 空操作         |

* 第十五章 进程地址空间

#+begin_src c
struct mm_struct {
	struct vm_area_struct * mmap;		/* list of VMAs */
	struct rb_root mm_rb;
	struct vm_area_struct * mmap_cache;	/* last find_vma result */
#ifdef CONFIG_MMU
	unsigned long (*get_unmapped_area) (struct file *filp,
				unsigned long addr, unsigned long len,
				unsigned long pgoff, unsigned long flags);
	void (*unmap_area) (struct mm_struct *mm, unsigned long addr);
#endif
	unsigned long mmap_base;		/* base of mmap area */
	unsigned long task_size;		/* size of task vm space */
	unsigned long cached_hole_size; 	/* if non-zero, the largest hole below free_area_cache */
	unsigned long free_area_cache;		/* first hole of size cached_hole_size or larger */
	pgd_t * pgd;
	atomic_t mm_users;			/* How many users with user space? */
	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
	int map_count;				/* number of VMAs */
	struct rw_semaphore mmap_sem;
	spinlock_t page_table_lock;		/* Protects page tables and some counters */

	struct list_head mmlist;		/* List of maybe swapped mm's.	These are globally strung
						 * together off init_mm.mmlist, and are protected
						 * by mmlist_lock
						 */


	unsigned long hiwater_rss;	/* High-watermark of RSS usage */
	unsigned long hiwater_vm;	/* High-water virtual memory usage */

	unsigned long total_vm, locked_vm, shared_vm, exec_vm;
	unsigned long stack_vm, reserved_vm, def_flags, nr_ptes;
	unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;

	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */

	/*
	 * Special counters, in some configurations protected by the
	 * page_table_lock, in other configurations by being atomic.
	 */
	struct mm_rss_stat rss_stat;

	struct linux_binfmt *binfmt;

	cpumask_t cpu_vm_mask;

	/* Architecture-specific MM context */
	mm_context_t context;

	/* Swap token stuff */
	/*
	 * Last value of global fault stamp as seen by this process.
	 * In other words, this value gives an indication of how long
	 * it has been since this task got the token.
	 * Look at mm/thrash.c
	 */
	unsigned int faultstamp;
	unsigned int token_priority;
	unsigned int last_interval;

	unsigned long flags; /* Must use atomic bitops to access the bits */

	struct core_state *core_state; /* coredumping support */
#ifdef CONFIG_AIO
	spinlock_t		ioctx_lock;
	struct hlist_head	ioctx_list;
#endif
#ifdef CONFIG_MM_OWNER
	/*
	 * "owner" points to a task that is regarded as the canonical
	 * user/owner of this mm. All of the following must be true in
	 * order for it to be changed:
	 *
	 * current == mm->owner
	 * current->mm != mm
	 * new_owner->mm == mm
	 * new_owner->alloc_lock is held
	 */
	struct task_struct *owner;
#endif

#ifdef CONFIG_PROC_FS
	/* store ref to file /proc/<pid>/exe symlink points to */
	struct file *exe_file;
	unsigned long num_exe_file_vmas;
#endif
#ifdef CONFIG_MMU_NOTIFIER
	struct mmu_notifier_mm *mmu_notifier_mm;
#endif
};
#+end_src
| Name     | Description                    |
|----------+--------------------------------|
| mm_users | 使用该地址的进程数目           |
| mm_count | 结构体本身引用计数             |
| mmap     | 内存区域,链表                  |
| mm_rb    | 和mmap一样描述内存区域，红黑树 |

所有mm_struct由mm_list域挂在双向链表中，头是init_mm
#+begin_src c
struct vm_area_struct {
	struct mm_struct * vm_mm;	/* The address space we belong to. */
	unsigned long vm_start;		/* Our start address within vm_mm. */
	unsigned long vm_end;		/* The first byte after our end address
					   within vm_mm. */

	/* linked list of VM areas per task, sorted by address */
	struct vm_area_struct *vm_next;

	pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
	unsigned long vm_flags;		/* Flags, see mm.h. */

	struct rb_node vm_rb;

	/*
	 * For areas with an address space and backing store,
	 * linkage into the address_space->i_mmap prio tree, or
	 * linkage to the list of like vmas hanging off its node, or
	 * linkage of vma in the address_space->i_mmap_nonlinear list.
	 */
	union {
		struct {
			struct list_head list;
			void *parent;	/* aligns with prio_tree_node parent */
			struct vm_area_struct *head;
		} vm_set;

		struct raw_prio_tree_node prio_tree_node;
	} shared;

	/*
	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
	 * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack
	 * or brk vma (with NULL file) can only be in an anon_vma list.
	 */
	struct list_head anon_vma_chain; /* Serialized by mmap_sem &
					  * page_table_lock */
	struct anon_vma *anon_vma;	/* Serialized by page_table_lock */

	/* Function pointers to deal with this struct. */
	const struct vm_operations_struct *vm_ops;

	/* Information about our backing store: */
	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
					   units, *not* PAGE_CACHE_SIZE */
	struct file * vm_file;		/* File we map to (can be NULL). */
	void * vm_private_data;		/* was vm_pte (shared mem) */
	unsigned long vm_truncate_count;/* truncate_count or restart_addr */

#ifndef CONFIG_MMU
	struct vm_region *vm_region;	/* NOMMU mapping region */
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
#endif
};


struct vm_operations_struct {
	void (*open)(struct vm_area_struct * area);
	void (*close)(struct vm_area_struct * area);
	int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);

	/* notification that a previously read-only page is about to become
	 * writable, if an error is returned it will cause a SIGBUS */
	int (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);

	/* called by access_process_vm when get_user_pages() fails, typically
	 * for use by special VMAs that can switch between memory and hardware
	 */
	int (*access)(struct vm_area_struct *vma, unsigned long addr,
		      void *buf, int len, int write);
#ifdef CONFIG_NUMA
	/*
	 * set_policy() op must add a reference to any non-NULL @new mempolicy
	 * to hold the policy upon return.  Caller should pass NULL @new to
	 * remove a policy and fall back to surrounding context--i.e. do not
	 * install a MPOL_DEFAULT policy, nor the task or system default
	 * mempolicy.
	 */
	int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);

	/*
	 * get_policy() op must add reference [mpol_get()] to any policy at
	 * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
	 * in mm/mempolicy.c will do this automatically.
	 * get_policy() must NOT add a ref if the policy at (vma,addr) is not
	 * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.
	 * If no [shared/vma] mempolicy exists at the addr, get_policy() op
	 * must return NULL--i.e., do not "fallback" to task or system default
	 * policy.
	 */
	struct mempolicy *(*get_policy)(struct vm_area_struct *vma,
					unsigned long addr);
	int (*migrate)(struct vm_area_struct *vma, const nodemask_t *from,
		const nodemask_t *to, unsigned long flags);
#endif
};
#+end_src

和物理页不同，VMA标志规定了读写执行等权限

- 页表
默认三级页表，struct mm_struct -> pgd_t -> pmd_t -> pte_t，缓存页面的硬件结构为TLB

Q: 页表是用于虚拟内存访问的，那它本身存在哪里？怎么访问？
A: 在mm_struct里的pgd,会存在cr3中，但存前会成物理地址给cr3，不然就套娃了
Q: 多级页表每一步中间查找用的是物理地址还是虚拟地址？
A: 物理地址

* 第十六章 页高速缓存和页回写

物理页到虚拟页是个一对多的映射,比如有多个进程的vm_area_struct映射到同一个页,但是只会有一个页缓存结构address_space
页高速缓存主要给I/O使用,但是它设计为一个通用结构
#+begin_src c
struct address_space {
	struct inode		*host;		/* owner: inode, block_device */
	struct radix_tree_root	page_tree;	/* radix tree of all pages */
	spinlock_t		tree_lock;	/* and lock protecting it */
	unsigned int		i_mmap_writable;/* count VM_SHARED mappings */
	struct prio_tree_root	i_mmap;		/* tree of private and shared mappings */
	struct list_head	i_mmap_nonlinear;/*list VM_NONLINEAR mappings */
	spinlock_t		i_mmap_lock;	/* protect tree, count, list */
	unsigned int		truncate_count;	/* Cover race condition with truncate */
	unsigned long		nrpages;	/* number of total pages */
	pgoff_t			writeback_index;/* writeback starts here */
	const struct address_space_operations *a_ops;	/* methods */
	unsigned long		flags;		/* error bits/gfp mask */
	struct backing_dev_info *backing_dev_info; /* device readahead, etc */
	spinlock_t		private_lock;	/* for use by the address_space */
	struct list_head	private_list;	/* ditto */
	struct address_space	*assoc_mapping;	/* ditto */
} __attribute__((aligned(sizeof(long))));
#+end_src

用户可以通过调用sync和fsync去写回脏页，同时flusher线程也被会周期性的唤醒去处理脏页

* 第十七章 设备与模块

/proc的结构比较混乱，/sys旨在解决这种混乱的结构，新的驱动程序都应在/sys下

| 块设备   | blkdev 可寻址，以块为单位，块大小随设备不同而不同，随机访问，通常被挂载为文件系统 |
| 字符设备 | cdev 不可寻址，数据流式访问，直接访问设备节点与字符设备交互                       |
| 网络设备 | 通过套接字访问                                                                    |

伪设备pseudo device - 非物理设备，用于访问内核
| 随机数发生器 | /dev/random、/dev/urandom |
| 空设备       | /dev/null                 |
| 零设备       | /dev/zero                 |
| 满设备       | /dev/full                 |
| 内存设备     | /dev/mem                  |

一个简单的kernel module的例了
#+begin_src c
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>

static int hello_init(void)
{
    printk(KERN_ALERT "I bear a charmed life.\n");
    return 0;
}

static void hello_exit(void)
{
    printk(KERN_ALERT "Out, out, brief candle.\n");
}

module_init(hello_init);
module_exit(hello_exit);
#+end_src

#+begin_src makefile
obj-m := hello.o
#+end_src

#+begin_src bash
make -C /mnt/hgfs/linux-2.6-git/linux-2.6/ SUBDIRS=$PWD modules
#+end_src

depmod生成模块依赖关系，depmod -A只生成新模块的依赖，生成的文件在/lib/modules/version/modules.dep中
insmod rmmod用于装载和卸载模块
modprobe modprobe -r也用于装载和卸载模块，同时会自动加载所依赖的模块

kernel编译用了kconfig系统
#+begin_src kconfig
config FISHING_POLE
	tristate "Fish Master 3000 support"
    default n
    help
        test kbuild system
#+end_src

EXPORT_SYMBOL用于导出符号

- 设备模型

kobject会嵌入到设备结构体中，包含kobject的结构可以使用kobject的功能
kt
kobject指向ktype，ktype定义了一些kobject的普遍特性，比如析构，sysfs
kobject同时归入kset集合，kset包含的kobject在sysfs中以独立的目录出现
#+begin_src c
struct kobject {
	const char		*name;
	struct list_head	entry;
	struct kobject		*parent; //父对象
	struct kset		*kset; //kobject集合
	struct kobj_type	*ktype; //普遍特性
	struct sysfs_dirent	*sd; //sysfs_dirent，在sys中表示
	struct kref		kref; //引用计数
	unsigned int state_initialized:1;
	unsigned int state_in_sysfs:1;
	unsigned int state_add_uevent_sent:1;
	unsigned int state_remove_uevent_sent:1;
	unsigned int uevent_suppress:1;
};
#+end_src

sys形成了driver的对象树,kobject是目录,文件是attribute

* 第十八章 调试

printk在任何上下文都可以调用
printk使用LOG_BUF_LEN的环形队列，默认是16KB，如果有大量消息，新消息会覆盖老消息
dmesg和/proc/kmsg获得内核消息
oops是内核错误时打印的日志，包括寄存器值和栈回溯
为了方便调试，会把符号表编入内核，CONFIG_KALLSYMS配置选项启用，这样栈回溯可以直接打出函数名，而不是地址
以下选项建议开启以便调试内核
#+begin_src bash
CONFIG_PREEMPT = y
CONFIG_DEBUG_KERNEL = y
CONFIG_KALLSYMS = y
CONFIG_DEBUG_SPINLOCK_SLEEP = y
#+end_src
kgdb也可以远程调试内核

- 技巧
a. 用uid来分离代码
#+begin_src c
if (current->uid != 7777) {
}
else {
//要调试的代码
}
#+end_src
b. 可以用全局变量控制代码流程
c. 加统计计数
d. 限制打印频率以防冲爆buffer
e. 知道代码从哪个提交开始可以用二分法

* 第十九章 可移植性

- 体系结构相关代码在arch/architecture/中
- 对于长度明确的数据类型(s8, u32),一般来说，如果头文件用户进程也会使用的话，不能直接用,要加两个_,如__u32
- 尽量显式保证自然对齐，而不让编译器来做, -Wpadded可以让gcc警告
- 最大公因子，假定任何事情都可能发生，任何潜在的约束也都存在
- 最小公约数，不要假定给定的内核特性是可用的，仅仅需要最小的体系结构功能

* 第二十章 补丁、开发和社区

代码风格 Documentation/CodingStyle
